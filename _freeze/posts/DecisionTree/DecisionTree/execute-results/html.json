{
  "hash": "633076bd415528beec1e2263bc93b040",
  "result": {
    "engine": "jupyter",
    "markdown": "---\njupyter: python3\n---\n\n::: {#c1c42cc5 .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nfrom scipy.special import xlogy\nfrom dataclasses import dataclass\nfrom typing import Literal\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n```\n:::\n\n\n::: {#aa8dd0a7 .cell execution_count=2}\n``` {.python .cell-code}\ndef check_ifreal(X: pd.Series) -> bool:\n\n    return X.dtype.name != 'category'\n\n\ndef entropy(Y: pd.Series) -> float:\n\n    vals = Y.value_counts(normalize=True)\n    return -np.sum(xlogy(vals, vals))\n\n\ndef gini_index(Y: pd.Series) -> float:\n\n    vals = Y.value_counts(normalize=True)\n    return 1 - np.sum(np.square(vals))\n\n\ndef information_gain(Y: pd.Series, attr: pd.Series, criterion) -> float:\n\n    criterion_func_map = {\n        'information_gain': entropy,\n        'gini_index': gini_index,\n        'mse': np.var\n    }\n\n    func = criterion_func_map[criterion]\n    value_before = func(Y)\n    split_value = None\n    if check_ifreal(attr):\n        split_value = opt_split_value(pd.DataFrame(attr), Y, attr.name)\n        value_after = Y.groupby(attr <= split_value).apply(lambda group: len(group) / len(Y) * func(group)).sum()\n    else:\n        value_after = Y.groupby(attr).apply(lambda group: len(group) / len(Y) * func(group)).sum()\n\n    return (value_before - value_after, split_value)\n\n\ndef opt_split_attribute(X: pd.DataFrame, y: pd.Series, criterion, features: pd.Series):\n\n    y = y if check_ifreal(y) else y.cat.codes\n    scores = {feature: information_gain(y, X[feature], criterion) for feature in features}\n\n    key = max(scores, key=lambda value: scores[value][0])\n    return key, scores[key][0], scores[key][1]\n\n\ndef real_variance(X: pd.DataFrame, y: pd.Series, value: np.float64 , attribute):\n\n    mask = (X[attribute] <= value)\n    var_left = np.var(y[mask]) * len(y[mask])\n    var_right = np.var(y[~mask]) * len(y[~mask])\n    return var_left + var_right\n\n\ndef opt_split_value(X: pd.DataFrame, y: pd.Series, attribute):\n\n    X = X.sort_values(by=[attribute])\n    check_values = [(X[attribute].iloc[i] + X[attribute].iloc[i+1]) / 2 for i in range(X.shape[0]-1)]\n\n    y = y if check_ifreal(y) else y.cat.codes\n    min_var = float('inf')\n    optimal_value = None\n\n    for value in check_values:\n        var = real_variance(X, y, value, attribute)\n        if var < min_var:\n            min_var = var\n            optimal_value = value\n\n    return optimal_value\n\n\ndef split_data(X: pd.DataFrame, y: pd.Series, attribute, value=None):\n\n    if not check_ifreal(X[attribute]):\n        unique_values = np.array(X[attribute].unique())\n        return [(X[X[attribute] == val], y[X[attribute] == val]) for val in unique_values], unique_values\n    else:\n        mask = (X[attribute] <= value)\n        return [(X[mask], y[mask]), (X[~mask], y[~mask])], value\n```\n:::\n\n\n::: {#7e409bf7 .cell execution_count=3}\n``` {.python .cell-code}\nnp.random.seed(42)\n\n@dataclass\nclass Node:\n\n    def __init__(self, attribute=None, depth=None, impurity=None, split_values=None, value=None):\n        self.attribute = attribute\n        self.value = value\n        self.split_values = split_values\n        self.impurity = impurity\n        self.depth = depth\n        self.subnodes = {}\n        self.is_leaf = False if value is None else True\n\n@dataclass\nclass DecisionTree:\n\n    criterion: Literal[\"information_gain\", \"gini_index\"]\n    max_depth: int  \n    def __init__(self, criterion, max_depth=5):\n        self.criterion = criterion\n        self.max_depth = max_depth\n        self.node = None\n\n    def fit(self, X: pd.DataFrame, y: pd.Series, depth=0) -> None:\n\n        attr, impurity, split_value = opt_split_attribute(X, y, self.criterion, X.columns.tolist())\n        if depth >= self.max_depth or X.shape[0] == 0 or y.nunique() == 1:\n            self.node = Node(depth=depth, impurity=impurity, value=y.median() if check_ifreal(y) else y.mode()[0])\n            return\n\n\n        splitted_data, values = split_data(X, y, attr, split_value)\n\n        self.node = Node(attr, depth, impurity, values)\n        if check_ifreal(X[attr]):\n            for value, data in zip([True, False], splitted_data):\n                subtree = DecisionTree(self.criterion, self.max_depth)\n                subtree.fit(data[0], data[1], depth + 1)\n                self.node.subnodes[value] = subtree\n        else:\n            for value, data in zip(values, splitted_data):\n                subtree = DecisionTree(self.criterion, self.max_depth)\n                subtree.fit(data[0], data[1], depth + 1)\n                self.node.subnodes[value] = subtree\n\n    def predict(self, X: pd.DataFrame) -> pd.Series:\n\n        def traverse_tree(x, node):\n            if node.is_leaf:\n                return node.value\n            attr = node.attribute\n            split_values = node.split_values\n            value = x[attr] <= split_values if check_ifreal(x) else x[attr]\n            if value in node.subnodes:\n                return traverse_tree(x, node.subnodes[value].node)\n            else:\n                raise ValueError(\n                    f\"Value {value} of {attr} not found in node {node}\")\n\n        predictions = X.apply(traverse_tree, axis=1, args=(self.node,))\n        return predictions\n\n    def plot(self, depth=0) -> None:\n        \"\"\"\n        Function to plot the tree\n\n        Output Example:\n        ?(X1 > 4)\n            Y: ?(X2 > 7)\n                Y: Class A\n                N: Class B\n            N: Class C\n        Where Y => Yes and N => No\n        \"\"\"\n\n        node = self.node\n\n        if node.is_leaf:\n            # If the node is a leaf node\n            print(f\"Class {node.value}\")\n        else:\n            # If the node is an internal node\n            attribute = node.attribute\n            split_values = node.split_values\n            if isinstance(split_values, np.ndarray):\n                # Discrete input\n                for value, subnode in node.subnodes.items():\n                    endline = '' if subnode.node.is_leaf else '\\n'\n                    print(f\"{' ' * depth * 4}?(column {attribute} == {value}): \", end=endline)\n                    subnode.plot(depth + 1)\n            else:\n                # Real input\n                print(f\"?({attribute} <= {split_values})\")\n                for value, subnode in node.subnodes.items():\n                    print(f\"{' ' * depth * 4}{value}: \", end=\"\")\n                    subnode.plot(depth + 1)\n```\n:::\n\n\n",
    "supporting": [
      "DecisionTree_files"
    ],
    "filters": [],
    "includes": {}
  }
}
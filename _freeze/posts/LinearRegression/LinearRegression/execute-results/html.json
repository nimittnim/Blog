{
  "hash": "158a7e8d6097cff4a88212987ca9c43b",
  "result": {
    "engine": "jupyter",
    "markdown": "---\njupyter: python3\n---\n\n::: {#4c076402 .cell execution_count=1}\n``` {.python .cell-code}\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import PolynomialFeatures\n```\n:::\n\n\n::: {#c74e7129 .cell execution_count=2}\n``` {.python .cell-code}\ndata = load_diabetes()\nX = data['data']\ny = data['target']\nnp.random.seed(32)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2 )\n```\n:::\n\n\nLinear Regression\n\n$\\theta = (X^{T}X)^{-1}X^{T}y$\n\n::: {#36d3f0f8 .cell execution_count=3}\n``` {.python .cell-code}\n# Derivative = 0 formula\nmodel1 = np.linalg.inv(X_train.T @ X_train) @ (X_train.T) @ y_train\ny_hat_1 = X_test @ model1\n```\n:::\n\n\n::: {#b223301c .cell execution_count=4}\n``` {.python .cell-code}\n# Sklearn Implementation\nmodel2 = LinearRegression()\nmodel2.fit(X_train, y_train)\ny_hat_2 = model2.predict(X_test)\n```\n:::\n\n\n::: {#cebacffa .cell execution_count=5}\n``` {.python .cell-code}\n# Singular Value Decomposition\nU, S, VT= np.linalg.svd(X_train, full_matrices=False)\nmodel3 = VT.T @ np.linalg.inv(np.diag(S)) @ U.T @ y_train\ny_hat_3 = X_test @ model3\n```\n:::\n\n\n::: {#29627805 .cell execution_count=6}\n``` {.python .cell-code}\nsno = np.arange(len(y_test))\nplt.plot(sno, y_test)\nplt.plot(sno, y_hat_1)\nplt.plot(sno, y_hat_2)\n# plt.plot(sno, y_hat_3) # gives same result as model1\n```\n\n::: {.cell-output .cell-output-display}\n![](LinearRegression_files/figure-html/cell-7-output-1.png){width=586 height=411}\n:::\n:::\n\n\n::: {#9459f8f7 .cell execution_count=7}\n``` {.python .cell-code}\n# Loss \nmean_squared_error(y_test,y_hat_1)\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\n25031.751595709775\n```\n:::\n:::\n\n\n::: {#5ca130a5 .cell execution_count=8}\n``` {.python .cell-code}\n# Sklearn implementation loss\nmean_squared_error(y_test,y_hat_2)\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\n2787.6434024439145\n```\n:::\n:::\n\n\nPolynomial Regression \\\nWe can improve the peroformance of Linear Regression by adding polynomials of features.\n\n::: {#0dc27024 .cell execution_count=9}\n``` {.python .cell-code}\ntrans = PolynomialFeatures(degree = 2)\nX_train_with_pf = trans.fit_transform(X_train)\nX_test_with_pf = trans.fit_transform(X_test)\n```\n:::\n\n\n::: {#8e8f188a .cell execution_count=10}\n``` {.python .cell-code}\nmodel1 = np.linalg.inv(X_train_with_pf.T @ X_train_with_pf) @ (X_train_with_pf.T) @ y_train\ny_hat_1 = X_test_with_pf @ model1\n```\n:::\n\n\n::: {#29879308 .cell execution_count=11}\n``` {.python .cell-code}\nmodel2 = LinearRegression()\nmodel2.fit(X_train, y_train)\ny_hat_2 = model2.predict(X_test)\n```\n:::\n\n\n::: {#ca4131eb .cell execution_count=12}\n``` {.python .cell-code}\nsno = np.arange(len(y_hat_1))\nplt.plot(sno,y_test)\nplt.plot(sno, y_hat_1)\nplt.plot(sno, y_hat_2)\n```\n\n::: {.cell-output .cell-output-display}\n![](LinearRegression_files/figure-html/cell-13-output-1.png){width=575 height=411}\n:::\n:::\n\n\n::: {#36a4a0c9 .cell execution_count=13}\n``` {.python .cell-code}\n# Loss \nmean_squared_error(y_test,y_hat_1)\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```\n3626.4586471045172\n```\n:::\n:::\n\n\nWe can see by adding polynomial features, the mean squared error reduced by a factor of 8! and gave a much better model\n\nMore Basis Functions \\\nSimilar to ading polynomials as features, we can add features which are function of given features\n\nChebyshev Polynomials\n\n$T_{0}(x) = 1$\\\n$T_{1}(x) = x$\\\n$T_{n+1}(x) = 2xT_{n}(x) - T_{n-1}(x)$\n\n::: {#0574784c .cell execution_count=14}\n``` {.python .cell-code}\n# Chebyshev Polynomials\ndef chebyshev_polynomial(x,degree):\n    n = degree\n    if degree == 0 :\n        return 1\n    elif degree == 1:\n        return x\n    else :\n        return (2*x)*chebyshev_polynomial(x,n-1) - chebyshev_polynomial(x,n-2)\n```\n:::\n\n\nLegendre Polynomial\n\n$P_{0}(x) = 1$\\\n$P_{1}(x) = x$\\\n$(n+1)P_{n+1}(x) = (2n+1)P_{n}(x) - P_{n-1}(x)$ \n\n::: {#712a463f .cell execution_count=15}\n``` {.python .cell-code}\n# Legendre Polynomial\ndef legendre_polynomial(x, degree):\n    n = degree\n    if degree == 0 :\n        return 1\n    elif degree == 1:\n        return x\n    else :\n        return ((1/n)*((2*n-1)*legendre_polynomial(x,n-1) - legendre_polynomial(x,n-2)))\n```\n:::\n\n\nBernstein Basis\n\n$B_{n,i}(x) = {n\\choose i} (1-x)^{n-i}x^{i} $\n\n",
    "supporting": [
      "LinearRegression_files"
    ],
    "filters": [],
    "includes": {}
  }
}
{
  "hash": "f3391cc0d03e1d4d434c0fc96ed9c65a",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Feed Forward Neural Network\"\nauthor: Nimitt\ndate: 2023-02-15\ncategories: [Machine Learning]\ndescription: \"Implementation of Feed Forward Neural Network from scratch\"\njupyter: python3\nexecute:\n  eval: false  \n---\n\n::: {#b56f2889 .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n```\n:::\n\n\nActivation Functions\n\n::: {#69a8819b .cell execution_count=2}\n``` {.python .cell-code}\n# Heaviside step function \n\ndef heavyside(x):\n    if (x >= 0):\n        return 1\n    else :\n        return 0\n        \n# Rectified error Linear Unit function\n\ndef relu(x):\n    return max(0,x)\n\n# Sigmoid function\n\ndef activation_sigmoid(t):\n        return 1/(1+np.exp(-t))\n\ndef sigmoid(x):\n    return 1/(1+np.exp(-x))\n\ndef softmax(t):\n    return np.exp(t)/np.sum(np.exp(t))\n\n# Gassian error Linear Unit GeLU function\n```\n:::\n\n\nNeural Network\n\n::: {#18454a03 .cell execution_count=3}\n``` {.python .cell-code}\n# Dataset\ndata = load_diabetes()\nX = data['data']\ny = data['target']\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)\n```\n:::\n\n\n::: {#72c7beda .cell execution_count=4}\n``` {.python .cell-code}\nclass NeuralNetwork :\n\n    def __init__(self):\n        self.layers_wieghts = []\n        self.layers_bias = []\n        self.layers_activations = []  \n\n    def activation_sigmoid(self, t):\n        return 1/(1+np.exp(-t))\n\n    def activation_sigmoid_derivative(self, t):\n        return np.exp(-t)/(1+np.exp(-t))**2\n    \n    def activation_relu(self,t):\n        return np.maximum(0,t)\n    \n    def activation_relu_derivative(self,t):\n        return np.where(t <= 0, 0, 1)\n\n    def activation_identity(self,t):\n        return t\n    \n    def activation_identity_derivative(self,t):\n        return 1\n\n    def loss(self, X_train, y_train):\n        y_hat = self.out(X_train)\n        return mean_squared_error(y_hat, y_train)\n\n    def loss_for_a_sample(self,x,y):\n        y_hat = self.out(x)\n        return np.mean((y_hat-y)**2)\n                                  \n    def integrate_layer(self,layer_index, x):\n        out = self.layers_wieghts[layer_index]@x + self.layers_bias[layer_index]\n        if (self.layers_activations[layer_index] == 's'):\n            return  self.activation_sigmoid(out)\n        elif (self.layers_activations[layer_index] == 'r'):\n            return  self.activation_relu(out)\n        elif (self.layers_activations[layer_index] == 'i'):\n            return  self.activation_identity(out)\n    \n    def add_layer(self, layer_size, activation):\n\n        if len(self.layers_activations) == 0:\n            self.layers_wieghts.append(np.random.rand(layer_size, layer_size))\n            self.layers_bias.append(np.random.rand(layer_size))\n            self.layers_activations.append(activation)\n        else :\n            self.layers_wieghts.append(np.random.rand(layer_size, len(self.layers_bias[-1])))\n            self.layers_bias.append(np.random.rand(layer_size))\n            self.layers_activations.append(activation)\n        \n    def update_weights(self, layer_index, delta_w):\n        self.layers_wieghts[layer_index]  = self.layers_wieghts[layer_index]  + delta_w\n\n    def update_bias(self, layer_index, delta_b):\n        self.layers_bias[layer_index]  = self.layers_bias[layer_index] + delta_b \n\n    def delete_layer(self):\n        if len(self.layers) > 1:\n            self.layers.pop()\n        else :\n            print(\"No Hidden Layers to delete\")\n\n    def derivative_of_activation_step(self,E, layer_index):\n        if self.layers_activations[layer_index] == 's':\n            return self.activation_sigmoid_derivative(E)\n        elif self.layers_activations[layer_index] == 'r':\n            return self.activation_relu_derivative(E)\n        elif self.layers_activations[layer_index] == 'i':\n            return self.activation_identity_derivative(E)\n\n    def backward(self, X_, y_, alpha):\n        activations = self.get_activations(X_)\n        l = len(self.layers_bias)\n\n        # Initiating with Output Layer\n        E_d = activations[l-1] - y_\n        E_d = E_d*self.derivative_of_activation_step(E_d, l-1)\n        delta_w = -(alpha/len(E_d))*2* E_d @ activations[l-2].T.reshape(-1, len(activations[l-2]))\n        delta_b = -(alpha/len(E_d))*2* E_d\n        delta_a_l = -alpha*2*E_d @ self.layers_wieghts[l-1]\n        delta_a_l = delta_a_l*self.derivative_of_activation_step(delta_a_l,l-1)\n        self.update_weights(l-1, delta_w)\n        self.update_bias(l-1, delta_b)\n        # print(activations)\n        # print(\"\\n\")\n        \n        # Hidden Layers\n        for i in range(l-2,0,-1):\n            delta_w = -((alpha/len(delta_a_l))*2* delta_a_l).reshape(len(delta_a_l),1) @ activations[i-1].T.reshape(-1, len(activations[i-1]))\n            delta_b = -(alpha/len(delta_a_l))*2* delta_a_l\n            self.update_weights(i, delta_w)\n            self.update_bias(i, delta_b)\n            delta_a_l = -(alpha/len(delta_a_l))*2*delta_a_l @ self.layers_wieghts[i]\n            delta_a_l = delta_a_l*self.derivative_of_activation_step(delta_a_l,i)\n        \n        # Input Layer\n        delta_w = -(alpha/len(delta_a_l))*2* delta_a_l.reshape(len(delta_a_l),1) @ X_.T.reshape(-1,len(X_))\n        delta_b = -(alpha/len(delta_a_l))*2* delta_a_l\n        self.update_weights(0, delta_w)\n        self.update_bias(0, delta_b)\n    \n    def get_activations(self, X_):\n        l = len(self.layers_bias)\n        activations = []\n        previous_out = self.integrate_layer(0, X_).copy()\n        activations.append(previous_out)\n        for i in range(1,l):\n            current_out = self.integrate_layer(i,previous_out).copy()\n            previous_out = current_out.copy()\n            activations.append(previous_out)\n        return activations\n\n    def forward(self, X_):\n        l = len(self.layers_bias)\n        previous_out = self.integrate_layer(0, X_).copy()\n        for i in range(1,l):\n            current_out = self.integrate_layer(i,previous_out).copy()\n            previous_out = current_out.copy()\n        return previous_out\n    \n    def train(self,alpha, X_train, y_train):\n        max_epochs = 1000\n        for e in range(max_epochs):\n            # out = self.predict(X_train)\n            # print(mean_squared_error(out,y_train),out[0])\n            for i in range(len(X_train)):\n                self.backward(X_train[i],y_train[i],alpha)\n            \n    def predict(self, X_test):\n        n = len(X_test)\n        m = len(self.layers_bias[-1])\n        prediction = np.empty((n,m))\n        for i in range(n):\n            out = self.forward(X_test[i])\n            for j in range(m):\n                prediction[i][j] = out[j]\n        return prediction\n\n```\n:::\n\n\n::: {#3c01778f .cell execution_count=5}\n``` {.python .cell-code}\nnn = NeuralNetwork()\nnn.add_layer(10,'r')\nnn.add_layer(1,'i')\n```\n:::\n\n\n::: {#dd0bccba .cell execution_count=6}\n``` {.python .cell-code}\nnn.forward(X_train[4])\n```\n:::\n\n\n::: {#82cc2fba .cell execution_count=7}\n``` {.python .cell-code}\nnn.train(0.0001, X_train, y_train) \n```\n:::\n\n\n::: {#09d946d3 .cell execution_count=8}\n``` {.python .cell-code}\ny_hat = nn.predict(X_test)\n```\n:::\n\n\n::: {#d0030d6e .cell execution_count=9}\n``` {.python .cell-code}\n# Plotting\nsr = np.arange(len(y_test))\nplt.plot(sr,y_test)\nplt.plot(sr,y_hat)\nplt.show()\n```\n:::\n\n\n::: {#2d784203 .cell execution_count=10}\n``` {.python .cell-code}\nmean_squared_error(y_hat, y_test)\n```\n:::\n\n\nPyTorch Implementation\n\n::: {#e18f3151 .cell execution_count=11}\n``` {.python .cell-code}\n# Dataset\ndata = load_diabetes()\nX = data['data']\ny = data['target']\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)\n\nX_train = torch.tensor(X_train, dtype=torch.float32)\ny_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\nX_test= torch.tensor(X_test, dtype=torch.float32)\ny_test = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)\n\n# Model\nmodel = nn.Sequential(\n    nn.Linear(10, 12),\n    nn.ReLU(),\n    nn.Linear(12, 8),\n    nn.ReLU(),\n    nn.Linear(8, 1),\n)\nprint(model)\n \n# train the model\nloss_fn   = nn.MSELoss()  \noptimizer = optim.Adam(model.parameters(), lr=0.001)\n \nn_epochs = 100\nbatch_size = 10\n \nfor epoch in range(n_epochs):\n    for i in range(0, len(X_train), batch_size):\n        Xbatch = X_train[i:i+batch_size]\n        y_pred = model(Xbatch)\n        ybatch = y_train[i:i+batch_size]\n        loss = loss_fn(y_pred, ybatch)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    print(f'Finished epoch {epoch}, current loss {loss}')\n```\n:::\n\n\n::: {#41567d92 .cell execution_count=12}\n``` {.python .cell-code}\ny_hat = model(X_test)\n```\n:::\n\n\n::: {#b3e4c2cf .cell execution_count=13}\n``` {.python .cell-code}\n# Plotting\nsr = np.arange(len(y_test))\nplt.plot(sr, np.array(y_test))\nplt.plot(sr,y_hat.detach().numpy())\nplt.show()\n```\n:::\n\n\n",
    "supporting": [
      "FeedForwardNeuralNetworks_files"
    ],
    "filters": [],
    "includes": {}
  }
}
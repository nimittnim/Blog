{
  "hash": "9b27eb2e8dc53b20fbc58d82a4aabad5",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Word Embeddings\"\nauthor: Nimitt\ndate: 2023-04-15\ncategories: [Machine Learning]\ndescription: \"Overview of WordEmbeddings Algorithm\"\njupyter: python3\nexecute:\n  eval: false  \n---\n\n\n\n**Word Embeddings**\n\nConvert tokens to vectors of real numbers based on usage and context.\n\n- Using Two Layer (Linear) Neural Network \n\nFeed $(n)$ one hot encoded samples call $X$  to Neural Network. Let $W_1(r,n)$ and $W_2(n,r)$ be weights of the first and second layer respectively. \n\n$Out = Softmax(W_{2}W_{1}X)$\n\nThe W1 weights are the representation vectors where each word has r dimensional vector.\n\n::: {#7eb88fa8 .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n```\n:::\n\n\n::: {#7542da29 .cell execution_count=2}\n``` {.python .cell-code}\nclass WordEmbeddings:\n\n    def __init__(self,m,r):\n\n        self.model = nn.Sequential(\n            nn.Linear(m,r),\n            nn.Linear(r,m),\n        )\n        self.loss_function = nn.CrossEntropyLoss()\n        self.optimizer = optim.Adam(self.model.parameters(),lr = 0.01)\n        self.unique_words = None\n        self.embedding = None\n\n    def one_hot_encode_word(self,word):\n\n        if (word in self.unique_words):\n            m = len(self.unique_words)\n            endcoded_word_index = self.unique_words.index(word)\n            encoded_word = np.zeros(m)\n            encoded_word[endcoded_word_index] = 1\n            return torch.tensor(encoded_word,dtype = torch.float32)\n        else :\n            print(\"Query word not in trained words\")\n    \n    def generate_encoded_dataset(self,words):\n\n        unique_words = list(set(words))\n        n = len(words)\n        m = len(unique_words)\n        X = np.zeros((n,m))\n        y = np.zeros((n,m))\n        for i in range(n-1):\n            j = unique_words.index(words[i])\n            j_next_word = unique_words.index(words[i+1])\n            X[i][j] = 1\n            y[i][j_next_word] = 1 \n        return torch.tensor(X,dtype = torch.float32),torch.tensor(y,dtype = torch.float32),unique_words\n        \n    def train(self,X,y):\n\n        # Training\n        max_epochs = 2000\n        # print(f'Initial Loss {self.loss_function(self.model(X),y)}')\n        for e in range(max_epochs):\n            y_hat = self.model(X)\n            loss = self.loss_function(y_hat,y)\n            self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step() \n        # print(f'Final Loss {self.loss_function(self.model(X),y)}')\n\n    def get_embedding(self,words):\n        \n        X,y,unique_words = self.generate_encoded_dataset(words)\n        self.unique_words = unique_words\n        self.train(X,y)\n        params = []\n        for param in self.model.parameters():\n            params.append(param)\n        embedding = np.array(params[0].detach()).T\n        self.embedding = embedding\n        return embedding\n    \n    def close_words(self, word, nof_words):\n        if (word in self.unique_words):\n            ind = self.unique_words.index(word)\n            closeness = np.sum((self.embedding - self.embedding[ind])**2,axis = 1)\n            word_indices = np.argsort(closeness)[0:nof_words]\n            words = []\n            for i in range(len(word_indices)):\n                words.append(self.unique_words[word_indices[i]])\n            return words\n        else :\n            print(\"Query word not in trained words\") \n\n    def next_word(self,word):\n\n        if (word in self.unique_words):\n            softmax = nn.Softmax(dim = 0)\n            out = torch.argmax(softmax(self.model(self.one_hot_encode_word(word))).detach())\n            return self.unique_words[out] \n        else :\n            print(\"Query word not in trained words\") \n\n```\n:::\n\n\n::: {#a54206f8 .cell execution_count=3}\n``` {.python .cell-code}\n# Generating the Dataset\nf = open(\"Datasets/Text.txt\",'r')\nwords = f.read().split()\nunique_words = list(set(words))\nn = len(words)\nm = len(unique_words)\n```\n:::\n\n\n::: {#8f543704 .cell execution_count=4}\n``` {.python .cell-code}\n# Training\nr = 20\nwe = WordEmbeddings(m,r)\ne = we.get_embedding(words)\n```\n:::\n\n\n::: {#5430e58e .cell execution_count=5}\n``` {.python .cell-code}\n# Next Word\nprint(we.next_word('AI'))\n```\n:::\n\n\n::: {#df49898a .cell execution_count=6}\n``` {.python .cell-code}\n# Close words\nwe.close_words('AI',10)\n```\n:::\n\n\n::: {#1a6c069c .cell execution_count=7}\n``` {.python .cell-code}\n# Visualizing the grouping of words based on generated vector representation\nplt.scatter(e[:,6],e[:,3])\nplt.xlabel(\"Component 1\")\nplt.ylabel(\"Component 2\")\n```\n:::\n\n\nWe can see the model tries to group words into different clusters.\n\n",
    "supporting": [
      "WordEmbeddings_files"
    ],
    "filters": [],
    "includes": {}
  }
}
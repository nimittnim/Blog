{
  "hash": "6a260114cd5a52a6435fd52f13885550",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Gradient Decent\"\nauthor: Nimitt\ndate: 2023-03-15\ncategories: [Machine Learning]\ndescription: \"Optimisation using Gradient Decent\"\njupyter: python3\nexecute:\n  eval: false  \n---\n\n\n\n\nGradient Decent\n\n$\\theta_{i+1} = \\theta_{i-1} - \\alpha \\frac{\\partial{loss(\\theta)}}{\\partial{\\theta}}$\n\n::: {#fd7c06c8 .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.metrics import mean_squared_error\n```\n:::\n\n\nExample 1 \n\n::: {#a2810e55 .cell execution_count=2}\n``` {.python .cell-code}\n# Creating the dataset\nn = 100\nX = np.random.rand(n)\ny = 2*X + 0.05*np.random.randn(n)\nplt.scatter(X,y)\nplt.xlabel('X')\nplt.ylabel('y')\nplt.show()\n```\n:::\n\n\n::: {#85adb870 .cell execution_count=3}\n``` {.python .cell-code}\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2)\n```\n:::\n\n\n::: {#195573e4 .cell execution_count=4}\n``` {.python .cell-code}\ndef loss_function(theta,X_tr, y_tr):\n    return mean_squared_error(theta*X_tr - y_tr)\n\ndef gradient_loss_function(theta, X_tr, y_tr):\n    return 2*(theta*np.sum(X_tr) - np.sum(y_tr))*np.sum(X_tr)\n\ndef model(theta, X_t):\n    return theta*X_t\n```\n:::\n\n\n::: {#573d4d97 .cell execution_count=5}\n``` {.python .cell-code}\nmax_iterations = 5\nalpha = 0.2\ntheta = np.random.rand() # initializing theta\nfor i in range(max_iterations):\n    print(theta)\n    theta = theta - 0.0002*gradient_loss_function(theta, X_train, y_train)  # learning theta\n    plt.scatter(X_test, y_test)\n    plt.plot(X_test, model(theta, X_test))\n    plt.show()\n```\n:::\n\n\nWe can see the performance of the model improves as we use gradient decent to train.\n\nRegular Gradient Decent\n\n::: {#75b6eb96 .cell execution_count=6}\n``` {.python .cell-code}\n# Creating the dataset\ndata = load_diabetes()\nX = data['data']\ny = data['target']\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)\n```\n:::\n\n\n::: {#c5fb9a94 .cell execution_count=7}\n``` {.python .cell-code}\n# loss function\ndef loss_function_gradient(theta, i, X_t, y_t): \n    return -2*np.sum((y_t - X_t@theta)*X_t[:,i])/len(X_t)\n\n# initializing theta\ntheta = np.random.rand(X_train.shape[1])\nalpha = 0.2\n\n# Regular Gradient Decent\nmax_iterations = 1000\nfor i in range(max_iterations):\n    for j in range(len(theta)):\n        theta[j] = theta[j] - alpha*(loss_function_gradient(theta, j, X_train, y_train))\n\ny_hat = X_test@theta\n```\n:::\n\n\n::: {#36202580 .cell execution_count=8}\n``` {.python .cell-code}\nsno = np.arange(len(y_test))\nplt.plot(sno, y_test)\nplt.plot(sno, y_hat)\nplt.show()\n```\n:::\n\n\nNow, we intend to add more polynomial features to train over\n\n::: {#0c17a9dc .cell execution_count=9}\n``` {.python .cell-code}\ntrans = PolynomialFeatures(degree = 2)\nX_train_with_pf = trans.fit_transform(X_train)\nX_test_with_pf = trans.fit_transform(X_test)\n```\n:::\n\n\n::: {#e8de8760 .cell execution_count=10}\n``` {.python .cell-code}\n# Training\n\n# loss function\ndef loss_function_gradient(theta, i, X_t, y_t): \n    return -2*np.sum((y_t - X_t@theta)*X_t[:,i])/len(X_t)\n\n# initializing theta\ntheta = np.random.rand(X_train_with_pf.shape[1])\nalpha = 0.2\n\n# Regular Gradient Decent\nmax_iterations = 1000\nfor i in range(max_iterations):\n    for j in range(len(theta)):\n        theta[j] = theta[j] - alpha*(loss_function_gradient(theta, j, X_train_with_pf, y_train))\ny_hat = X_test_with_pf@theta\n```\n:::\n\n\n::: {#85641b1b .cell execution_count=11}\n``` {.python .cell-code}\nsno = np.arange(len(y_test))\nplt.plot(sno, y_test)\nplt.plot(sno, y_hat)\nplt.show()\n```\n:::\n\n\n::: {#7f69afc2 .cell execution_count=12}\n``` {.python .cell-code}\n# loss\nmean_squared_error(y_test, y_hat)\n```\n:::\n\n\nStochastic Gradient Decent\n\n::: {#3612d710 .cell execution_count=13}\n``` {.python .cell-code}\n# creating the dataset\n\ndata = load_diabetes()\nX = data['data']\ny = data['target']\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)\n```\n:::\n\n\n::: {#934d8b7c .cell execution_count=14}\n``` {.python .cell-code}\n# loss function\ndef loss_function_stochastic_gradient(theta, i, X_t, j, y_t): \n    return -2*((y_t[j] - X_t[j]@theta)*X_t[j][i])/len(X_t)\n\n# initializing theta\ntheta = np.random.rand(X_train.shape[1])\nalpha = 0.2\n\n# Stochastic Gradient Decent\nmax_epochs = 1000\nfor i in range(max_epochs):\n    # k = np.random.randint(0,len(X_train))\n    for k in range(len(X_train)):\n        for j in range(len(theta)):\n            theta[j] = theta[j] - alpha*(loss_function_stochastic_gradient(theta, j, X_train,k, y_train))\n\ny_hat = X_test@theta\n```\n:::\n\n\n::: {#3956139b .cell execution_count=15}\n``` {.python .cell-code}\n# plotting prediction\nsno = np.arange(len(y_test))\nplt.plot(sno, y_test)\nplt.plot(sno, y_hat)\nplt.show()\n```\n:::\n\n\nAdding more features into the dataset\n\n::: {#be5dd1b0 .cell execution_count=16}\n``` {.python .cell-code}\n# adding polynomial features\ntrans = PolynomialFeatures(degree = 2)\nX_train_with_pf = trans.fit_transform(X_train)\nX_test_with_pf = trans.fit_transform(X_test)\n```\n:::\n\n\nStochastic Gradient Decent\n\n::: {#e6d1b827 .cell execution_count=17}\n``` {.python .cell-code}\n# loss function\ndef loss_function_stochastic_gradient(theta, i, X_t, j, y_t): \n    return -2*((y_t[j] - X_t[j]@theta)*X_t[j][i])/len(X_t)\n\n# initializing theta\ntheta = np.random.rand(X_train_with_pf.shape[1])\nalpha = 0.2\n\n# Stochastic Gradient Decent\n\nmax_iterations = 1000\nfor i in range(max_iterations):\n    for k in range(len(X_train_with_pf)):\n        for j in range(len(theta)):\n            theta[j] = theta[j] - alpha*(loss_function_stochastic_gradient(theta, j, X_train_with_pf,k, y_train))\n\ny_hat = X_test_with_pf@theta\n```\n:::\n\n\n::: {#4c47d9c1 .cell execution_count=18}\n``` {.python .cell-code}\n# plotting prediction\nsno = np.arange(len(y_test))\nplt.plot(sno, y_test)\nplt.plot(sno, y_hat)\nplt.show()\n```\n:::\n\n\n::: {#db7dabd9 .cell execution_count=19}\n``` {.python .cell-code}\n# loss\nmean_squared_error(y_test, y_hat)\n```\n:::\n\n\n",
    "supporting": [
      "GradientDecent_files"
    ],
    "filters": [],
    "includes": {}
  }
}
{
  "hash": "906025f5c7ccf07d02592610c697c73b",
  "result": {
    "engine": "jupyter",
    "markdown": "---\njupyter: python3\n---\n\n\n\n\n**Positional Encoding**\n\nFor Sequential data, we need to have some kind of context thing that tells how the data is related sequentially. The LSTMs used the previous cell state to influence the current input thus building context. There is one more technique introduced in Attention is all you need - Positional Encoding that does the same. \n\n::: {#412c15f1 .cell execution_count=1}\n``` {.python .cell-code}\nimport torch.nn as nn\nimport torch\nimport numpy as np\n```\n:::\n\n\n::: {#a529a350 .cell execution_count=2}\n``` {.python .cell-code}\ndef getPositionEncoding(seq_len, d, n=10000):  # 10000 in attention paper\n    P = torch.empty((seq_len, d))\n    for k in range(seq_len):\n        for i in range(d//2):\n            denominator = np.power(n, 2*i/d)\n            P[k, 2*i] = np.sin(k/denominator)\n            P[k, 2*i+1] = np.cos(k/denominator)\n    return P\n \nP = getPositionEncoding(seq_len=4, d=4, n=100)\nprint(P)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor([[ 0.0000,  1.0000,  0.0000,  1.0000],\n        [ 0.8415,  0.5403,  0.0998,  0.9950],\n        [ 0.9093, -0.4161,  0.1987,  0.9801],\n        [ 0.1411, -0.9900,  0.2955,  0.9553]])\n```\n:::\n:::\n\n\n",
    "supporting": [
      "PositionalEncoding_files"
    ],
    "filters": [],
    "includes": {}
  }
}
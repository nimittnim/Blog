{
  "hash": "c1eed2122447a96c3b3ef9bec9e3ee5a",
  "result": {
    "engine": "jupyter",
    "markdown": "---\njupyter: python3\n---\n\n\n\n\n**Transformers**\n\nAttention is All you need ! : https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwiUu_Xi2NqEAxXoslYBHUlFA9sQFnoECA0QAQ&url=https%3A%2F%2Farxiv.org%2Fabs%2F1706.03762&usg=AOvVaw2ceXGQohV5Kx51VSkfkG08&opi=89978449\n\nTransformers use the concept of Attention to gain context.\n\n::: {#533af59b .cell execution_count=1}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n```\n:::\n\n\n::: {#c8cde29f .cell execution_count=2}\n``` {.python .cell-code}\nplt.figure(figsize=(30,30))\nplt.imshow(plt.imread(\"Random/Transformer.png\"))\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Transformers_files/figure-html/cell-3-output-1.png){width=1872 height=2263}\n:::\n:::\n\n\n::: {#901daea9 .cell execution_count=3}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n```\n:::\n\n\n::: {#9b320787 .cell execution_count=4}\n``` {.python .cell-code}\nclass WordEmbeddings:\n\n    def __init__(self,m,r):\n\n        self.model = nn.Sequential(\n            nn.Linear(m,r),\n            nn.Linear(r,m),\n        )\n        self.loss_function = nn.CrossEntropyLoss()\n        self.optimizer = optim.Adam(self.model.parameters(),lr = 0.01)\n        self.unique_words = None\n        self.embedding = None\n\n    def one_hot_encode_word(self,word):\n\n        if (word in self.unique_words):\n            m = len(self.unique_words)\n            endcoded_word_index = self.unique_words.index(word)\n            encoded_word = np.zeros(m)\n            encoded_word[endcoded_word_index] = 1\n            return torch.tensor(encoded_word,dtype = torch.float32)\n        else :\n            print(\"Query word not in trained words\")\n    \n    def generate_encoded_dataset(self,words):\n\n        unique_words = list(set(words))\n        n = len(words)\n        m = len(unique_words)\n        X = np.zeros((n,m))\n        y = np.zeros((n,m))\n        for i in range(n-1):\n            j = unique_words.index(words[i])\n            j_next_word = unique_words.index(words[i+1])\n            X[i][j] = 1\n            y[i][j_next_word] = 1 \n        return torch.tensor(X,dtype = torch.float32),torch.tensor(y,dtype = torch.float32),unique_words\n        \n    def train(self,X,y):\n\n        # Training\n        max_epochs = 2000\n        # print(f'Initial Loss {self.loss_function(self.model(X),y)}')\n        for e in range(max_epochs):\n            y_hat = self.model(X)\n            loss = self.loss_function(y_hat,y)\n            self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step() \n        # print(f'Final Loss {self.loss_function(self.model(X),y)}')\n\n    def get_embedding(self,words):\n        \n        X,y,unique_words = self.generate_encoded_dataset(words)\n        self.unique_words = unique_words\n        self.train(X,y)\n        params = []\n        for param in self.model.parameters():\n            params.append(param)\n        embedding = np.array(params[0].detach()).T\n        self.embedding = embedding\n        return embedding\n    \n    def close_words(self, word, nof_words):\n        if (word in self.unique_words):\n            ind = self.unique_words.index(word)\n            closeness = np.sum((self.embedding - self.embedding[ind])**2,axis = 1)\n            word_indices = np.argsort(closeness)[0:nof_words]\n            words = []\n            for i in range(len(word_indices)):\n                words.append(self.unique_words[word_indices[i]])\n            return words\n        else :\n            print(\"Query word not in trained words\") \n\n    def next_word(self,word):\n\n        if (word in self.unique_words):\n            softmax = nn.Softmax(dim = 0)\n            out = torch.argmax(softmax(self.model(self.one_hot_encode_word(word))).detach())\n            return self.unique_words[out] \n        else :\n            print(\"Query word not in trained words\") \n```\n:::\n\n\n",
    "supporting": [
      "Transformers_files"
    ],
    "filters": [],
    "includes": {}
  }
}
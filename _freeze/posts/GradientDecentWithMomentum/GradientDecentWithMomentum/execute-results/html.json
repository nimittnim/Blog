{
  "hash": "d5e7f543dd24e09f1e45cf45f950e932",
  "result": {
    "engine": "jupyter",
    "markdown": "---\njupyter: python3\n---\n\n\n\n\nGradient Decent With Momentum\n\nThe previous update in parameters is indicator for curvature of the loss surface. So, we use the previous update along with current gradient to update the parameters. \n\n$\\theta_{t} = \\theta_{t-1} + \\alpha \\frac{\\partial L(\\theta)}{\\partial \\theta} (\\theta_{t-1}) + m \\delta \\theta_{t-1}$\n\nwhere $ 0 < m < 1 $\n\nGreater the m, more the infuence of previous update on current gradient.\n\n::: {#3e4aaa06 .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n```\n:::\n\n\n::: {#97ad4cbd .cell execution_count=2}\n``` {.python .cell-code}\nn = 100\n\ndef Loss(theta):\n    return theta**2\n\ndef grdaient_Loss(theta):\n    return 2*theta \n```\n:::\n\n\n::: {#336dae68 .cell execution_count=3}\n``` {.python .cell-code}\nmax_iterations = 25\nalpha = 0.1\n\ntheta = 5\nthetas_gradient = np.empty(25)\nprint(\"Gradient Decent\")\nfor i in range(max_iterations):\n    theta = theta - alpha*grdaient_Loss(theta)\n    thetas_gradient[i] = theta\n    # print(f'Current theta = {theta}, f(current_theta) = {Loss(theta)}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nGradient Decent\n```\n:::\n:::\n\n\nUsing Momentum\n\n::: {#ad80759c .cell execution_count=4}\n``` {.python .cell-code}\nmax_iterations = 25\nalpha = 0.1\nm = 0.35\ntheta = 5\ndeltheta = 0\n\nprint(\"Gradient Decent with Momentum\")\nthetas_momentum = np.empty(25)\nfor i in range(max_iterations):\n    deltheta = alpha*grdaient_Loss(theta) + m*deltheta\n    theta = theta - deltheta\n    thetas_momentum[i] = theta\n    # print(f'Current theta = {theta}, f(current_theta) = {Loss(theta)}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nGradient Decent with Momentum\n```\n:::\n:::\n\n\n::: {#09246178 .cell execution_count=5}\n``` {.python .cell-code}\n# True Minima  \nX = np.linspace(-100,100,10000) \ny = Loss(X)\nminima = X[np.argmin(y)]\n```\n:::\n\n\n::: {#0d7e1d92 .cell execution_count=6}\n``` {.python .cell-code}\nsn = np.arange(max_iterations)\nexpected_minima = minima*np.ones(max_iterations)\nplt.plot(sn, thetas_gradient, label = \"GD\")\nplt.plot(sn,thetas_momentum, label = \"GD with Momentum\")\nplt.plot(sn,expected_minima, label = \"Expected Minima\")\nplt.legend()\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Current Theta\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](GradientDecentWithMomentum_files/figure-html/cell-7-output-1.png){width=593 height=429}\n:::\n:::\n\n\n",
    "supporting": [
      "GradientDecentWithMomentum_files"
    ],
    "filters": [],
    "includes": {}
  }
}
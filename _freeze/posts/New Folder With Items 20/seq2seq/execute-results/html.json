{
  "hash": "0dd910433fd2a8cdbc2f7be6f86c181e",
  "result": {
    "engine": "jupyter",
    "markdown": "---\njupyter: python3\n---\n\n\n\n\n**SEQ2SEQ**\n\nUse LSTM and word embeddings to translate one coding to other.\n\n::: {#7271a25f .cell execution_count=1}\n``` {.python .cell-code}\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch\nimport numpy as np\n```\n:::\n\n\n::: {#4e27420b .cell execution_count=2}\n``` {.python .cell-code}\nclass WordEmbeddings:\n\n    def __init__(self,m,r):\n\n        self.model = nn.Sequential(\n            nn.Linear(m,r),\n            nn.Linear(r,m),\n        )\n        self.loss_function = nn.CrossEntropyLoss()\n        self.optimizer = optim.Adam(self.model.parameters(),lr = 0.01)\n        self.unique_words = None\n\n    def one_hot_encode_word(self,word):\n\n        if (word in self.unique_words):\n            m = len(self.unique_words)\n            endcoded_word_index = self.unique_words.index(word)\n            encoded_word = np.zeros(m)\n            encoded_word[endcoded_word_index] = 1\n            return torch.tensor(encoded_word,dtype = torch.float32)\n        else :\n            print(\"Query word not in trained words\")\n    \n    def generate_encoded_dataset(self,words):\n\n        unique_words = list(set(words))\n        n = len(words)\n        m = len(unique_words)\n        X = np.zeros((n,m))\n        y = np.zeros((n,m))\n        for i in range(n-1):\n            j = unique_words.index(words[i])\n            j_next_word = unique_words.index(words[i+1])\n            X[i][j] = 1\n            y[i][j_next_word] = 1 \n        return torch.tensor(X,dtype = torch.float32),torch.tensor(y,dtype = torch.float32)\n        \n    def train(self,X,y):\n        \n        max_epochs = 2000\n        # print(f'Initial Loss {self.loss_function(self.model(X),y)}')\n        for e in range(max_epochs):\n            y_hat = self.model(X)\n            loss = self.loss_function(y_hat,y)\n            self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step()\n        # print(f'Final Loss {self.loss_function(self.model(X),y)}')\n\n    def get_embedding(self,words):\n\n        unique_words = list(set(words))\n        self.unique_words = unique_words\n        \n        X,y = self.generate_encoded_dataset(words)\n        self.train(X,y)\n        params = []\n        for param in self.model.parameters():\n            params.append(param)\n        embedding = np.array(params[0].detach()).T\n        return embedding\n\n    def next_word(self,word):\n\n        if (word in self.unique_words):\n            softmax = nn.Softmax(dim = 0)\n            out = np.array(softmax(self.model(self.one_hot_encode_word(word))).detach())\n            return self.unique_words[np.where(out == np.max(out))[0][0]]    \n        else :\n            print(\"Query word not in trained words\") \n            \n    def vec2word(self,vec):\n        ind = np.argmin(np.array(vec.detach()))\n        return self.unique_words[ind]\n```\n:::\n\n\n::: {#14666733 .cell execution_count=3}\n``` {.python .cell-code}\n# LSTM cell\n\nclass LSTMcell:\n\n    def __init__(self,m,r):\n\n        # percentage long term to remember\n        self.w_input_1 = nn.Parameter(torch.rand(r,m),requires_grad=True)\n        self.w_short_1 = nn.Parameter(torch.rand(r,r),requires_grad=True)\n        self.b_1 = nn.Parameter(torch.rand(r,1),requires_grad=True)\n        # sigma\n\n        # percentage potential memory to remember\n        self.w_input_2 = nn.Parameter(torch.rand(r,m),requires_grad=True)\n        self.w_short_2 = nn.Parameter(torch.rand(r,r),requires_grad=True)\n        self.b_2 = nn.Parameter(torch.rand(r,1),requires_grad=True)\n        # sigma\n\n        # Potential Long term memory for current input\n        self.w_input_3 = nn.Parameter(torch.rand(r,m),requires_grad=True)\n        self.w_short_3 = nn.Parameter(torch.rand(r,r),requires_grad=True)\n        self.b_3 = nn.Parameter(torch.rand(r,1),requires_grad=True)\n        # tanh\n\n        # New short\n        self.w_input_4 = nn.Parameter(torch.rand(r,m),requires_grad=True)\n        self.w_short_4 = nn.Parameter(torch.rand(r,r),requires_grad=True)\n        self.b_4 = nn.Parameter(torch.rand(r,1),requires_grad=True)\n        # sigmoid\n        self.parameters = torch.tensor([self.w_input_1,self.w_input_2,self.w_input_3,self.w_input_4,\n                           self.w_short_1,self.w_short_2,self.w_short_3,self.w_short_4,\n                           self.b_1,self.b_2,self.b_3,self.b_4],requires_grad=True)\n\n\n    def integrate(self, input, short, long):\n        # percentage long term to remember\n        \n        o1 = torch.matmul(self.w_input_1, input)\n        o2 = torch.matmul(self.w_short_1, short)\n        o3 = torch.mul(long,torch.sigmoid(o1+o2 + self.b_1))\n\n        # percentage potential memory to remember\n        o4 = torch.matmul(self.w_input_2, input)\n        o5 = torch.matmul(self.w_short_2, short) \n        o6 = torch.sigmoid(o4+o5 + self.b_2)\n\n        # Potential Long term memory for current input\n        o7 = torch.matmul(self.w_input_3, input)\n        o8 = torch.matmul(self.w_short_3, short)\n        o9 = torch.tanh(o7+o8+ self.b_3) \n\n        newLong = o3 + torch.mul(o6,o9)\n        \n\n        # new short\n        o10 = torch.matmul(self.w_input_4, input)\n        o11 = torch.matmul(self.w_short_4, short)\n        o12 = torch.sigmoid(o11+o10 + self.b_4)\n        \n        newShort = torch.mul(o12,(torch.tanh(newLong)))\n\n        return newShort,newLong\n```\n:::\n\n\n::: {#217adac0 .cell execution_count=4}\n``` {.python .cell-code}\nclass LSTM:\n\n    def __init__(self):\n        self.layers = []\n        self.ms = []\n        self.rs = []\n        self.parameters = []\n        self.loss_function = nn.MSELoss()\n    \n    def add_layer(self,m,r,n): \n        # m : Input dimension \n        # n :number of cells in the layer \n        # r : output dimension\n        self.layers.append([])  \n        self.ms.append(m)\n        self.rs.append(r)   \n        for i in range(n):\n            newcell = LSTMcell(m,r)\n            self.layers[-1].append(newcell)\n            self.parameters.append(newcell.parameters)\n\n    def forward(self, input):\n        l = len(self.layers)\n        short = torch.zeros((l, self.rs[0],1), dtype = torch.float32)\n        long = torch.zeros((l,self.rs[0],1),dtype = torch.float32)\n        for i in range(len(input)):\n            current_input = input[i].unsqueeze(1)\n            for j in range(l):\n                short_,long_ = short[j],long[j]\n                for lstm_cell in self.layers[j]:\n                    short_,long_ = lstm_cell.integrate(current_input,short_,long_)\n                short[j],long[j] = short_,long_\n                current_input = short_\n        return short_\n    \n    def out(self,input):\n        out = []\n        for i in range(len(input)):\n            out.append(self.forward(input[i]))\n        return torch.tensor(out,requires_grad=True).unsqueeze(-1)\n    \n    def train(self,X,y,max_epochs):\n        optimizer = optim.Adam(self.parameters)\n        for i in range(max_epochs):\n            out = self.out(X)\n            loss = self.loss_function(out,y)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n```\n:::\n\n\n::: {#f8c9f093 .cell execution_count=5}\n``` {.python .cell-code}\nclass Seq2Seq:\n\n    def __init__(self,m):\n        r = 20\n        self.we = WordEmbeddings(m,r)\n        self.encoder = LSTM()\n        self.decoder = LSTM()\n        self.feed_forward = nn.Sequential(\n            nn.Linear(r,r),\n            nn.Softmax()\n        )\n        self.loss_function = nn.CrossEntropyLoss()\n\n    def encode(self,input):\n        return self.encoder.forward(input,0,0)\n    \n    def feed(self,input):\n        return self.feed_forward(input)\n    \n    def decode(self, short,long):\n        EOS = self.we.one_hot_encode_word(\"EOS\")\n        input = EOS.copy()\n        translation = []\n        while True:\n            out = self.feed(self.decoder.forward(input, short, long))\n            translation.append(out)\n            if (self.we.unique_words(max(out)) == \"EOS\"):\n                break\n        return torch.tensor(translation,dtype = torch.flaot32)\n    \n    def generate_embeddings(self,X,y):\n        X_embeddings = self.we.get_embeddings(X)\n        y_embeddings = self.we.get_embeddings(y)\n        return torch.tensor(X_embeddings,dtype=torch.float32),torch.tensor(y_embeddings,dtype=torch.float32)\n    \n    def forward(self, X,y):\n        inp = self.generate_embeddings(X,y)\n        context_vec =  self.encode(inp)\n        translation = self.decode(context_vec[0],context_vec[1])\n        return translation\n    \n    def train(self,X,y):\n        optimizer = nn.optim(self.paramters())\n        loss_function = self.loss_function()\n        y_hat = self.forward(X,y)\n        return\n```\n:::\n\n\n",
    "supporting": [
      "seq2seq_files"
    ],
    "filters": [],
    "includes": {}
  }
}
{
  "hash": "ef2ce22c74cf881bba828a97c8bd4409",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Matrix Factorization\"\nauthor: Nimitt\ndate: 2023-03-15\ncategories: [Machine Learning]\ndescription: \"Lower Represention of a matrix\"\njupyter: python3\nexecute:\n  eval: false  \n---\n\n\n\nMatrix Completion\n\nWe intend to complete the Matrix with unkown entries using Matrix decomposotion where we learn decomposed matrices using Gradient Decent and Nested Linear Regression\n\nMatrix Factorization\n\nFor a matrix $A_{n,m}$ learn $W_{n,r}$ and $H_{r,m}$ such that,\n\n$ A = WH$\n\nMethod 1 : Gradient Decent\n\n::: {#1644d66d .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as  np\nimport matplotlib.pyplot as plt\n```\n:::\n\n\n::: {#66010d02 .cell execution_count=2}\n``` {.python .cell-code}\n# Creating A\nfrom matplotlib.image import imread\n\nX = imread(\"Datasets/puppy.jpg\")\nA = np.mean(X,-1)\nimg = plt.imshow(A)\nplt.set_cmap('gray')\nplt.show()\n```\n:::\n\n\n::: {#9511faf8 .cell execution_count=3}\n``` {.python .cell-code}\n# Normalizing the data\n\nA_transformed = A/255\n```\n:::\n\n\n::: {#4d369dd7 .cell execution_count=4}\n``` {.python .cell-code}\nimg = plt.imshow(A_transformed)\nplt.set_cmap('gray')\nplt.show()\n```\n:::\n\n\n::: {#4795db59 .cell execution_count=5}\n``` {.python .cell-code}\n# Training\n\n# Gradients\n\ndef loss_gradient_wrt_W(W,H,A):\n    lg_matrix = np.empty(W.shape)\n    N  = (A.shape[0]*A.shape[1])\n    for j1 in range(W.shape[0]):\n        M = A[j1] - W[j1]@H\n        for j2 in range(W.shape[1]): \n            lg_matrix[j1][j2] = -2*( M @ H[j2])/ N\n    return lg_matrix\n\ndef loss_gradient_wrt_H(W,H,A):\n    lg_matrix = np.empty(H.shape)\n    N  = (A.shape[0]*A.shape[1])\n    for j2 in range(H.shape[0]):\n        M = A[:,j2] - W@H[:,j2]\n        for j1 in range(H.shape[1]):\n            lg_matrix[j1][j2] = -(2* M @ W[:,j1])/ N\n    return lg_matrix\n\n\ndef loss (A, W, H):\n    return np.sum((A - W@H)**2)/(A.shape[0]*A.shape[1])\n\n# intializing W and H\nr = 100\nn = A.shape[0]\nm = A.shape[1]\nW = 0.01*np.random.rand(n,r)\nH = 0.01*np.random.rand(r,m)\n\nmax_iterations = 200\n\nalpha = 150\n\nfor i in range(max_iterations):\n    W = W - alpha*loss_gradient_wrt_W(W,H,A_transformed)\n    H = H - alpha*loss_gradient_wrt_H(W,H,A_transformed)\n    img = plt.imshow(W@H)\n    if(i%5 == 0):\n        print(loss(A_transformed, W,H))\n        print(max((W@H)[0]))\n        plt.set_cmap('gray')\n        plt.show()\n```\n:::\n\n\n::: {#2cfb4764 .cell execution_count=6}\n``` {.python .cell-code}\nA_constructed = W@H\n```\n:::\n\n\n::: {#4a7694c8 .cell execution_count=7}\n``` {.python .cell-code}\nimg = plt.imshow(A_constructed)\nplt.set_cmap('gray')\nplt.show()\n```\n:::\n\n\nIf the given matrix A is incomplete, we simply don't take into account those elements in thee loss function for learning W and H. After learning W and H, the completed version of A is W @ H.\n\nMethod 2 : Alternating Least Squares\n\nLet's first use the A matrix as it is :\n\n::: {#a76182a2 .cell execution_count=8}\n``` {.python .cell-code}\n# intializing W and H\nr = 100\nn = A.shape[0]\nm = A.shape[1]\nW = np.random.rand(n,r)\nH = np.random.rand(r,m)\n\n# training\n\nmax_iterations = 100\n\nfor _ in range(max_iterations):\n    # learning H\n    for i in range(H.shape[1]):\n        H[:,i] = np.linalg.inv((W.T@W))@W.T@A[:,i]\n    # learning W\n    for j in range(W.shape[0]):\n        W[j] = np.linalg.inv(H@H.T)@H@A[j]\nconstructed_A = W@H\n```\n:::\n\n\n::: {#7af2d838 .cell execution_count=9}\n``` {.python .cell-code}\nimg = plt.imshow(constructed_A)\nplt.set_cmap('gray')\nplt.show()\n```\n:::\n\n\nDropping some values in A :\n\n::: {#7b3605c5 .cell execution_count=10}\n``` {.python .cell-code}\nmask = np.random.rand(A.shape[0], A.shape[1])\nfor i in range(A.shape[0]):\n    for j in range(A.shape[1]):\n        if mask[i][j] < 0.5:\n            mask[i][j] = 1\n        else :\n            mask[i][j] = 0\nM = np.ma.masked_array(A,mask)\n```\n:::\n\n\n::: {#e1c9c1f9 .cell execution_count=11}\n``` {.python .cell-code}\nA_incomplete = M.filled(np.nan)\n```\n:::\n\n\n::: {#0d8ed6d8 .cell execution_count=12}\n``` {.python .cell-code}\nimg = plt.imshow(A_incomplete)\nplt.set_cmap('gray')\nplt.show()\n```\n:::\n\n\n::: {#8ad73d92 .cell execution_count=13}\n``` {.python .cell-code}\ndef plot_image(A):\n    plt.imshow(A)\n    plt.set_cmap('gray')\n    plt.show()\n```\n:::\n\n\n::: {#0dd1e390 .cell execution_count=14}\n``` {.python .cell-code}\n# Reconstructing\n\n# intializing W and H\nr = 100\nn = A.shape[0]\nm = A.shape[1]\nW = np.random.rand(n,r)\nH = np.random.rand(r,m)\n\n# training\n\nmax_iterations = 3\n\nfor _ in range(max_iterations):\n    # learning H\n    for i in range(H.shape[1]):\n        keep = ~np.isnan(A_incomplete[:,i])\n        M = np.linalg.inv((W.T@W))@W.T\n        for i1 in range(H.shape[0]):\n            H[:,i][i1] = M[i1][keep]@A_incomplete[:,i][keep]\n    # learning W\n    for j in range(W.shape[0]):\n        keep = ~np.isnan(A_incomplete[j])\n        M = (np.linalg.inv(H@H.T)@H)\n        for j1 in range(W.shape[1]):\n            W[j][j1] = M[j1][keep]@A_incomplete[j][keep]\n    plot_image(W@H)\nconstructed_A = W@H\n```\n:::\n\n\n::: {#71e14cb7 .cell execution_count=15}\n``` {.python .cell-code}\nplot_image(constructed_A)\n```\n:::\n\n\n",
    "supporting": [
      "MatrixFactorization_files"
    ],
    "filters": [],
    "includes": {}
  }
}
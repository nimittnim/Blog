{
  "hash": "9b4e584be3ca8fa9418cc1fe2fbb9806",
  "result": {
    "engine": "jupyter",
    "markdown": "---\njupyter: python3\n---\n\n\n\n\n**Image Reconstruction using Matrix Factorization and Linear Regression with RFF**\n\nImporting Libraries\n\n::: {#762e079e .cell execution_count=1}\n``` {.python .cell-code}\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport torch.optim as optim\nfrom sklearn.kernel_approximation import RBFSampler\nimport warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport os\nfrom einops import rearrange\nfrom sklearn import preprocessing\nos.environ['CUDA_LAUNCH_BLOCKING'] = '1'\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n%config InlineBackend.figure_format = 'retina'\n```\n:::\n\n\nProcessing Functions\n\n::: {#f449a685 .cell execution_count=2}\n``` {.python .cell-code}\ndef mask_image_patch(img,x,y,z,patch_size):\n    img_copy = img.clone()\n    for i in range(patch_size):\n        for j in range(patch_size):\n                for k in range(z):\n                    img_copy[x+i][y+j][k] = torch.nan\n    return img_copy\n\ndef mask_image_random(img,patch_size):\n    img_copy = img.clone()\n    n = len(img)\n    m = len(img[0])\n    random_i = np.random.choice(n*m,patch_size*patch_size,replace=False)\n    for i in random_i:\n        for j in range(3):\n            img_copy[i//n][i%m] = torch.nan\n    return img_copy\n\ndef create_mask(t,x,y,patch_size):\n    mask = torch.full(t.shape,True)\n    z = t.shape[2]\n    for i in range(patch_size):\n        for j in range(patch_size):\n                for k in range(z):\n                    mask[x+i][y+j][k] = False\n    return mask\n\ndef create_coordinate_map(img):\n    height, width, num_channels = img.shape\n    X = torch.empty((img.shape[0],img.shape[1],2))\n    \n    for i in range(height):\n        for j in range(width):\n            X[i][j][0] = i\n            X[i][j][1] = j\n    return X.reshape(-1,2)\n\ndef stack_itself(t, n):\n    stacked_t = t.unsqueeze(1).expand(-1, n, -1)\n    return stacked_t.squeeze()\n\ndef scale(img):\n    img_flatted = img.reshape(-1,1)\n    scaler_X = preprocessing.MinMaxScaler().fit(img_flatted)\n    img_scaled = scaler_X.transform(img_flatted).reshape(img.shape)\n    img_scaled = torch.tensor(img_scaled)\n    img_scaled = img_scaled.float()\n    return img_scaled\n\ndef fill_patch(original_img,reconstructed_img_patch,x,y,patch_size):\n    reconstructed_img = original_img.clone()\n    for i in range(patch_size):\n        for j in range(patch_size):\n                for k in range(3):\n                    reconstructed_img[x+i][y+j][k] = reconstructed_img_patch[i][j][k]\n    return reconstructed_img\n```\n:::\n\n\nCreating Dataset\n\n::: {#06dd3ad9 .cell execution_count=3}\n``` {.python .cell-code}\nimg = torchvision.io.read_image('Datasets/dog.jpg')\nimg_scaled = scale(img)\ncropped_img = torchvision.transforms.functional.crop(img_scaled,600,800,300,300)\ncropped_img = rearrange(cropped_img,'c h w -> h w c')\ncropped_img = torch.tensor(cropped_img,dtype = torch.float32)\noriginal_img = cropped_img.clone()\n```\n:::\n\n\nMatrix Factorization\n\n::: {#ffb0e17b .cell execution_count=4}\n``` {.python .cell-code}\ndef factorize_matrix(A, r):\n    mask = ~torch.isnan(A)\n    m,n = A.shape\n    W = torch.rand(m,r,requires_grad=True)\n    H = torch.rand(r,n,requires_grad=True)\n    optimizer = optim.Adam([W,H],lr = 0.01)\n\n    prev_loss = float('inf')  \n    epsilon = 1e-6\n    max_epochs = 1000\n\n    for i in range(max_epochs):\n        loss = torch.norm((A - torch.mm(W, H))[mask])\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        if abs(prev_loss - loss.item()) < epsilon:\n            break\n        prev_loss = loss.item()\n\n    return W, H, loss\n\ndef image_reconstrunction_matrix_factorization(masked_img,r):\n    W,H,loss = factorize_matrix(rearrange(masked_img, 'h w c -> h (w c)'), r)\n    reconstructed_img = torch.mm(W,H).detach()\n    reconstructed_img = reconstructed_img.reshape(masked_img.shape[0],masked_img.shape[1],masked_img.shape[2])\n\n    # reconstructed_img = scale(reconstructed_img)\n    return reconstructed_img\n```\n:::\n\n\nLinear Regression + RFF\n\n::: {#2e5ee30f .cell execution_count=5}\n``` {.python .cell-code}\n# LR + RFF\ndef plot_reconstructed_and_original_image(original_img, masked_img, reconstructed_img, title=\"\"):\n    \"\"\"\n    net: torch.nn.Module\n    masked_img: torch.Tensor of shape (num_samples, 2)\n    Y: torch.Tensor of shape (num_samples, 3)\n    \"\"\"\n    fig = plt.figure(figsize=(6, 4))\n    gs = gridspec.GridSpec(1, 3, width_ratios=[1, 1, 1])\n\n    amasked_img0 = plt.subplot(gs[0])\n    amasked_img1 = plt.subplot(gs[1])\n    amasked_img2 = plt.subplot(gs[2])\n\n    amasked_img0.imshow(reconstructed_img)\n    amasked_img0.set_title(\"Reconstructed Image\")\n    \n    amasked_img1.imshow(original_img.cpu())\n    amasked_img1.set_title(\"Original Image\")\n\n    amasked_img2.imshow(masked_img.cpu())\n    amasked_img2.set_title(\"Masked Image\")\n\n    \n    # for a in [amasked_img0, amasked_img1, amasked_img2]:\n    #     a.amasked_imgis(\"off\")\n    plt.show()\n\n# def create_rff_features(masked_img, num_features, sigma):\n#     from sklearn.kernel_appromasked_imgimation import RBFSampler\n#     rff = RBFSampler(n_components=num_features, gamma=1/(2 * sigma**2))\n#     masked_img = masked_img.cpu().numpy()\n#     masked_img = rff.fit_transform(masked_img)\n#     return torch.tensor(masked_img, dtype=torch.float32)\n\ndef train(net, lr, masked_img, Y, mamasked_img_epochs=2000):\n    \"\"\"\n    net: torch.nn.Module\n    lr: float\n    masked_img: torch.Tensor of shape (known_pimasked_imgels, 2) // (masked_img,y)\n    Y: torch.Tensor of shape (known_pimasked_imgels, 3) // (r,g,b)\n    \"\"\"\n\n    criterion = nn.MSELoss()\n    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n    outputs = net(masked_img)\n    loss = criterion(outputs, Y)\n    \n    prev_loss = float('inf')  \n    epsilon = 1e-5\n    prev_loss = float('inf')\n    mamasked_img_epochs = 10000\n    \n    for i in range(mamasked_img_epochs):\n        optimizer.zero_grad()\n        outputs = net(masked_img)\n        loss = criterion(outputs, Y)\n        loss.backward()\n        optimizer.step()\n\n        if abs(prev_loss - loss.item()) < epsilon:\n            break\n        prev_loss = loss.item()\n\n\n\nclass LinearModel(nn.Module):\n    def __init__(self, in_features, out_features):\n        super(LinearModel, self).__init__()\n        self.linear = nn.Linear(in_features, out_features)\n        \n    def forward(self, masked_img):\n        return self.linear(masked_img)\n    \n\ndef image_reconstrunction_linear_rff(masked_img,n_components,sigma = 0.008):\n    \n    # y = original_img.clone().reshape(-1,3)\n    # masked_img = create_coordinate_map(original_img)\n    # mask = ~torch.isnan(masked_img).reshape(-1,3)\n    # masked_img_train = masked_img[mask[:,0:2]].reshape(-1,2)\n    # y_train = y[mask].reshape(-1,3)\n\n    # # Without RFF\n    # # net = LinearModel(2,3)\n    # # train(net,0.01,masked_img_train,y_train,1000)\n    # # plot_reconstructed_and_original_image(original_img, net, masked_img, title=\"Reconstructed Image\")\n\n    # # With RFF\n    # features = 15000\n    # masked_img_rff = create_rff_features(masked_img, features, 0.008)\n    # mask_rff = stack_itself(mask[:,0].unsqueeze(1),features)\n    # masked_img_rff_train = masked_img_rff[mask_rff].reshape(-1,features)\n    # y_rff_train = y[mask].reshape(-1,3)\n\n    \n    # netrff = LinearModel(masked_img_rff_train.shape[1], 3)\n    # train(netrff, 0.005, masked_img_rff_train, y_rff_train, 1000)\n    # height, width, num_channels = original_img.shape\n    # netrff.eval()\n    # with torch.no_grad():\n    #     reconstructed_img = netrff(masked_img_rff).reshape(height,width,num_channels)\n    # return reconstructed_img\n\n    masked_img_flat = rearrange(original_img, 'h w c -> (h w) c').cpu().numpy()\n    RFF = RBFSampler(n_components=n_components, gamma=1/(2 * sigma**2))\n    masked_img_transformed = RFF.fit_transform(masked_img_flat)\n\n    from sklearn.linear_model import LinearRegression\n    linear_regressor = LinearRegression()\n    linear_regressor.fit(masked_img_transformed, masked_img_flat)\n    masked_img_reconstructed = linear_regressor.predict(masked_img_transformed)\n\n    masked_img_reconstructed = masked_img_reconstructed.reshape(masked_img.shape[0], masked_img.shape[1], 3)\n    return torch.tensor(masked_img_reconstructed, dtype=torch.float32).to(device)\n```\n:::\n\n\nMatrix Factorization using alternation Least Squares\n\nData Compression\n\n::: {#6745fdca .cell execution_count=6}\n``` {.python .cell-code}\ndef compress_patch(img_patch, r):\n    W,H,loss = factorize_matrix(rearrange(img_patch, 'h w c -> h (w c)'), r)\n    reconstructed_img = torch.mm(W,H).detach()\n    reconstructed_img = reconstructed_img.reshape(img_patch.shape[0],img_patch.shape[1],img_patch.shape[2])\n\n    # reconstructed_img = scale(reconstructed_img)\n    return reconstructed_img\n```\n:::\n\n\nMetrics\n\n::: {#5852204c .cell execution_count=7}\n``` {.python .cell-code}\ndef mse(original_img,reconstructed_img):\n    sum = torch.tensor(0)\n    for i in range(3):\n        sum = sum + (nn.functional.mse_loss(original_img[:,:,i],reconstructed_img[:,:,i]))\n    return sum/3\n```\n:::\n\n\n::: {#8a20650b .cell execution_count=8}\n``` {.python .cell-code}\ndef psnr(original_img,reconstructed_img):\n    psnr = torch.tensor(0)\n    for i in range(3):\n        maxi = torch.max(original_img[:,:,i])\n        mse = nn.functional.mse_loss(original_img[:,:,i],reconstructed_img[:,:,i])\n        psnr = psnr + (20*torch.log10(maxi/mse))\n    return psnr/3\n```\n:::\n\n\nReconstructions\n\n::: {#62959878 .cell execution_count=9}\n``` {.python .cell-code}\n# 1a MF\npatch_size = 30\n# x_y_s = [[10,10,3],[90,150,3],[140,60,3]]\nx_y_s = [[90,150,3]]\n# titles = [\"A. 1 color patch\", \"B. 2 color patch\", \"C. 5 color patch\"]\ntitle = \"Matrix Factorisation 30*30 rectangular patch\"\nreconstructed_images_1 = []\n\nfor i in range(len(x_y_s)):\n    print(title)\n    masked_img = mask_image_patch(original_img,x_y_s[i][0],x_y_s[i][1],3,patch_size)\n    reconstructed_img = image_reconstrunction_matrix_factorization(masked_img,100)\n    reconstructed_images_1.append(reconstructed_img)\n    mask = torch.isnan(masked_img)\n    original_patch = original_img[mask].reshape(patch_size,patch_size,3)\n    reconstructed_patch = reconstructed_img[mask].reshape(patch_size,patch_size,3)\n    fig,axs = plt.subplots(2,2)\n    axs[0][0].imshow(masked_img)\n    axs[0][1].imshow(reconstructed_img)\n    axs[1][0].imshow(original_patch)\n    axs[1][1].imshow(reconstructed_patch)\n    plt.show()\n    print(\"MSE for reconstruction : \",mse(original_patch, reconstructed_patch))\n    print(\"PSNR for reconstruction : \",psnr(original_patch, reconstructed_patch))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMatrix Factorisation 30*30 rectangular patch\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](ImageReconstruction_files/figure-html/cell-10-output-3.png){width=506 height=416}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nMSE for reconstruction :  tensor(0.0432)\nPSNR for reconstruction :  tensor(26.2322)\n```\n:::\n:::\n\n\n::: {#a3bf06aa .cell execution_count=10}\n``` {.python .cell-code}\n# 1a RFF\npatch_size = 30\n# x_y_s = [[10,10],[90,150],[140,60]]\nx_y_s = [[90,150]]\nreconstructed_images_2 = []\n\nfor i in range(len(x_y_s)):\n    print(\"Linear Regression + RFF for 30*30 rectangular patch\")\n    masked_img = mask_image_patch(original_img,x_y_s[i][0],x_y_s[i][1],3,patch_size)\n    reconstructed_img = image_reconstrunction_linear_rff(masked_img,5000)\n    mask = torch.isnan(masked_img)\n    original_patch = original_img[mask].reshape(patch_size,patch_size,3)\n    reconstructed_patch = reconstructed_img[mask].reshape(patch_size,patch_size,3)\n    fig,axs = plt.subplots(2,2)\n    axs[0][0].imshow(masked_img)\n    axs[0][1].imshow(reconstructed_img)\n    axs[1][0].imshow(original_patch)\n    axs[1][1].imshow(reconstructed_patch)\n    plt.show()\n    print(\"MSE for reconstruction : \",mse(original_patch, reconstructed_patch))\n    print(\"PSNR for reconstruction : \",psnr(original_patch, reconstructed_patch))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear Regression + RFF for 30*30 rectangular patch\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](ImageReconstruction_files/figure-html/cell-11-output-3.png){width=506 height=416}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nMSE for reconstruction :  tensor(0.0016)\nPSNR for reconstruction :  tensor(55.2938)\n```\n:::\n:::\n\n\n::: {#422b1772 .cell execution_count=11}\n``` {.python .cell-code}\n# 1b MF\n\npatch_size = 30\ntitles = [\"Random Missing pixels MF\"]\nreconstructed_images_3 = []\n\nprint(titles[0])\nmasked_img = mask_image_random(original_img, patch_size)\nreconstructed_img = image_reconstrunction_matrix_factorization(masked_img,50)\nreconstructed_images_3.append(reconstructed_img)\nfig,axs = plt.subplots(1,2)\naxs[0].imshow(masked_img)\naxs[1].imshow(reconstructed_img)\nplt.show()\nprint(\"MSE for reconstruction : \",mse(original_img, reconstructed_img))\nprint(\"PSNR for reconstruction : \",psnr(original_img, reconstructed_img))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRandom Missing pixels MF\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](ImageReconstruction_files/figure-html/cell-12-output-3.png){width=575 height=283}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nMSE for reconstruction :  tensor(0.0006)\nPSNR for reconstruction :  tensor(64.7727)\n```\n:::\n:::\n\n\n::: {#7675abfd .cell execution_count=12}\n``` {.python .cell-code}\n# 1b RFF\npatch_size = 30\ntitles = [\"Random Missing pixels RFF\"]\nreconstructed_images_4 = []\n\nprint(titles[0])\nmasked_img = mask_image_random(original_img, patch_size)\nreconstructed_img = image_reconstrunction_linear_rff(masked_img,8000)\nreconstructed_images_4.append(reconstructed_img)\nfig,axs = plt.subplots(1,2)\naxs[0].imshow(masked_img)\naxs[1].imshow(reconstructed_img)\nplt.show()\nprint(\"MSE for reconstruction : \",mse(original_img, reconstructed_img))\nprint(\"PSNR for reconstruction : \",psnr(original_img, reconstructed_img))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRandom Missing pixels RFF\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](ImageReconstruction_files/figure-html/cell-13-output-3.png){width=575 height=283}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nMSE for reconstruction :  tensor(0.0036)\nPSNR for reconstruction :  tensor(48.9278)\n```\n:::\n:::\n\n\nWe can see, we get better reconstruction using Linear Regression than Matrix Factorization\n\n::: {#817f0146 .cell execution_count=13}\n``` {.python .cell-code}\n# 2a MF\n\nx_start = 50\ny_start = 50\npatch_sizes = [20,40,60,80,100]\nreconstructed_images_5 = []\nmses= []\npsnrs = []\n\nfor i in range(len(patch_sizes)):\n    print(\"---------------------------\")\n    print(f\"Patch Size  : {patch_sizes[i]}\")\n    patch_size = patch_sizes[i]\n    masked_img = mask_image_patch(original_img,x_start,y_start,3,patch_sizes[i])\n    reconstructed_img = image_reconstrunction_matrix_factorization(masked_img,100)\n    reconstructed_images_5.append(reconstructed_img)\n    mask = torch.isnan(masked_img)\n    fig,axs = plt.subplots(2,2)\n    axs[0][0].imshow(masked_img)\n    axs[0][1].imshow(reconstructed_img)\n    axs[1][0].imshow(original_img[mask].reshape(patch_size,patch_size,3))\n    axs[1][1].imshow(reconstructed_img[mask].reshape(patch_size,patch_size,3))\n    plt.show()\n    ms = mse(original_img, reconstructed_img)\n    ps = psnr(original_img, reconstructed_img)\n    mses.append(ms)\n    psnrs.append(ps)\n    print(\"MSE for reconstruction : \",ms)\n    print(\"PSNR for reconstruction : \",ps)\n    print(\"---------------------------\")\n\nprint(\"---------------------------\")\nplt.plot(patch_sizes,mses)\nplt.xlabel(\"Patch size\")\nplt.ylabel(\"MSE for reconstruction\")\nprint(\"---------------------------\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n---------------------------\nPatch Size  : 20\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](ImageReconstruction_files/figure-html/cell-14-output-3.png){width=506 height=416}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nMSE for reconstruction :  tensor(0.0003)\nPSNR for reconstruction :  tensor(70.0460)\n---------------------------\n---------------------------\nPatch Size  : 40\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](ImageReconstruction_files/figure-html/cell-14-output-6.png){width=506 height=416}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nMSE for reconstruction :  tensor(0.0010)\nPSNR for reconstruction :  tensor(60.2608)\n---------------------------\n---------------------------\nPatch Size  : 60\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](ImageReconstruction_files/figure-html/cell-14-output-9.png){width=506 height=416}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nMSE for reconstruction :  tensor(0.0028)\nPSNR for reconstruction :  tensor(51.0024)\n---------------------------\n---------------------------\nPatch Size  : 80\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](ImageReconstruction_files/figure-html/cell-14-output-12.png){width=506 height=416}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nMSE for reconstruction :  tensor(0.0040)\nPSNR for reconstruction :  tensor(48.0764)\n---------------------------\n---------------------------\nPatch Size  : 100\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](ImageReconstruction_files/figure-html/cell-14-output-15.png){width=506 height=416}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nMSE for reconstruction :  tensor(0.0097)\nPSNR for reconstruction :  tensor(40.3289)\n---------------------------\n---------------------------\n---------------------------\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](ImageReconstruction_files/figure-html/cell-14-output-17.png){width=606 height=429}\n:::\n:::\n\n\n::: {#5c73e26d .cell execution_count=14}\n``` {.python .cell-code}\n# 2b MF\nx_start = 50\ny_start = 50\n# n1 = [30]\npatch_sizes = [20,40,60,80,100]\nreconstructed_images_6 = []\nmses = []\npsnrs = []\n\nfor i in range(len(patch_sizes)):\n    print(\"---------------------------\")\n    print(f\"Patch Size  : {patch_sizes[i]}\")\n    masked_img = mask_image_random(original_img,patch_sizes[i])\n    reconstructed_img = image_reconstrunction_matrix_factorization(masked_img,50)\n    reconstructed_images_6.append(reconstructed_img)\n    fig,axs = plt.subplots(1,2)\n    axs[0].imshow(masked_img)\n    axs[1].imshow(reconstructed_img)\n    plt.show()\n    ms = mse(original_img, reconstructed_img)\n    ps = psnr(original_img, reconstructed_img)\n    mses.append(ms)\n    psnrs.append(ps)\n    print(\"MSE for reconstruction : \",ms)\n    print(\"PSNR for reconstruction : \",ps)\n    print(\"---------------------------\")\n\nprint(\"---------------------------\")\nplt.plot(patch_sizes,mses)\nplt.xlabel(\"Patch size\")\nplt.ylabel(\"MSE for reconstruction\")\nplt.show()\nprint(\"---------------------------\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n---------------------------\nPatch Size  : 20\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](ImageReconstruction_files/figure-html/cell-15-output-3.png){width=575 height=283}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nMSE for reconstruction :  tensor(0.0006)\nPSNR for reconstruction :  tensor(64.8470)\n---------------------------\n---------------------------\nPatch Size  : 40\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](ImageReconstruction_files/figure-html/cell-15-output-6.png){width=575 height=283}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nMSE for reconstruction :  tensor(0.0006)\nPSNR for reconstruction :  tensor(64.6616)\n---------------------------\n---------------------------\nPatch Size  : 60\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](ImageReconstruction_files/figure-html/cell-15-output-9.png){width=575 height=283}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nMSE for reconstruction :  tensor(0.0006)\nPSNR for reconstruction :  tensor(64.2025)\n---------------------------\n---------------------------\nPatch Size  : 80\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](ImageReconstruction_files/figure-html/cell-15-output-12.png){width=575 height=283}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nMSE for reconstruction :  tensor(0.0006)\nPSNR for reconstruction :  tensor(64.3217)\n---------------------------\n---------------------------\nPatch Size  : 100\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](ImageReconstruction_files/figure-html/cell-15-output-15.png){width=575 height=283}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nMSE for reconstruction :  tensor(0.0006)\nPSNR for reconstruction :  tensor(64.0207)\n---------------------------\n---------------------------\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](ImageReconstruction_files/figure-html/cell-15-output-17.png){width=623 height=429}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n---------------------------\n```\n:::\n:::\n\n\nData Compression Reconstructions\n\n::: {#e3cbf891 .cell execution_count=15}\n``` {.python .cell-code}\npatch_size = 50\nx_y_s = [[10,10],[90,150],[140,60]]\nrs = [5, 10, 25, 50]\ntitles = [\"A. 1 color patch\", \"B. 2 color patch\", \"C. 5 color patch\"]\n\ndef border_patch(original_img,x,y,patch_size):\n    bordered_img = original_img.clone()\n    for i in range(patch_size):\n        for k in range(3):\n            bordered_img[x+i][y][k] = torch.nan \n            bordered_img[x+i][y+patch_size][k] = torch.nan \n\n    for j in range(patch_size):\n        for k in range(3):\n            bordered_img[x][y+j][k] = torch.nan \n            bordered_img[x+patch_size][y+j][k] = torch.nan \n\n    return bordered_img\n\nmses = np.empty((len(x_y_s),len(rs)))\npses = np.empty((len(x_y_s),len(rs)))\n\nfor i in range(len(x_y_s)):\n    print(\"---------------------------\")\n    print(titles[i])\n    for j in range(len(rs)):\n        print(\"r : \",rs[j])\n        mask = create_mask(original_img, x_y_s[i][0], x_y_s[i][1],patch_size)\n        img_patch = original_img[~mask].reshape(patch_size,patch_size,3)\n        reconstructed_img_patch  = image_reconstrunction_matrix_factorization(img_patch,rs[j])\n        reconstructed_img = fill_patch(original_img,reconstructed_img_patch,x_y_s[i][0],x_y_s[i][1],patch_size)\n        fig,axs = plt.subplots(1,3)\n        axs[0].imshow(original_img)\n        axs[1].imshow(border_patch(original_img,x_y_s[i][0],x_y_s[i][1],patch_size))\n        axs[2].imshow(reconstructed_img)\n        plt.show()\n        ms = mse(img_patch, reconstructed_img_patch)\n        ps = psnr(img_patch, reconstructed_img_patch)\n        print(\"MSE for reconstruction : \",ms)\n        print(\"PSNR for reconstruction : \",ps)\n        mses[i][j] = ms\n        pses[i][j] = ps\n    print(\"---------------------------\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n---------------------------\nA. 1 color patch\nr :  5\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](ImageReconstruction_files/figure-html/cell-16-output-2.png){width=575 height=199}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nMSE for reconstruction :  tensor(9.0553e-06)\nPSNR for reconstruction :  tensor(98.6452)\nr :  10\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](ImageReconstruction_files/figure-html/cell-16-output-4.png){width=575 height=199}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nMSE for reconstruction :  tensor(5.9841e-06)\nPSNR for reconstruction :  tensor(102.1143)\nr :  25\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](ImageReconstruction_files/figure-html/cell-16-output-6.png){width=575 height=199}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nMSE for reconstruction :  tensor(8.2938e-06)\nPSNR for reconstruction :  tensor(99.2837)\nr :  50\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](ImageReconstruction_files/figure-html/cell-16-output-8.png){width=575 height=199}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nMSE for reconstruction :  tensor(1.0909e-05)\nPSNR for reconstruction :  tensor(96.9057)\n---------------------------\n---------------------------\nB. 2 color patch\nr :  5\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](ImageReconstruction_files/figure-html/cell-16-output-11.png){width=575 height=199}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nMSE for reconstruction :  tensor(0.0005)\nPSNR for reconstruction :  tensor(65.8513)\nr :  10\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](ImageReconstruction_files/figure-html/cell-16-output-14.png){width=575 height=199}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nMSE for reconstruction :  tensor(0.0001)\nPSNR for reconstruction :  tensor(77.2119)\nr :  25\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](ImageReconstruction_files/figure-html/cell-16-output-16.png){width=575 height=199}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nMSE for reconstruction :  tensor(2.6907e-05)\nPSNR for reconstruction :  tensor(90.3394)\nr :  50\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](ImageReconstruction_files/figure-html/cell-16-output-18.png){width=575 height=199}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nMSE for reconstruction :  tensor(1.7875e-05)\nPSNR for reconstruction :  tensor(93.9191)\n---------------------------\n---------------------------\nC. 5 color patch\nr :  5\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](ImageReconstruction_files/figure-html/cell-16-output-20.png){width=575 height=199}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nMSE for reconstruction :  tensor(0.0026)\nPSNR for reconstruction :  tensor(50.8034)\nr :  10\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](ImageReconstruction_files/figure-html/cell-16-output-23.png){width=575 height=199}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nMSE for reconstruction :  tensor(0.0008)\nPSNR for reconstruction :  tensor(60.5929)\nr :  25\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](ImageReconstruction_files/figure-html/cell-16-output-25.png){width=575 height=199}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nMSE for reconstruction :  tensor(3.3340e-05)\nPSNR for reconstruction :  tensor(88.6006)\nr :  50\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](ImageReconstruction_files/figure-html/cell-16-output-27.png){width=575 height=199}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nMSE for reconstruction :  tensor(1.1926e-05)\nPSNR for reconstruction :  tensor(97.5402)\n---------------------------\n```\n:::\n:::\n\n\n::: {#9d621e49 .cell execution_count=16}\n``` {.python .cell-code}\nprint(\"MSE for Different Cases\")\nmse_dataframe = pd.DataFrame(mses,index = ['1 color','2 colors','5 colors'],columns = rs)\nmse_dataframe\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMSE for Different Cases\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=16}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>5</th>\n      <th>10</th>\n      <th>25</th>\n      <th>50</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1 color</th>\n      <td>0.000009</td>\n      <td>0.000006</td>\n      <td>0.000008</td>\n      <td>0.000011</td>\n    </tr>\n    <tr>\n      <th>2 colors</th>\n      <td>0.000451</td>\n      <td>0.000122</td>\n      <td>0.000027</td>\n      <td>0.000018</td>\n    </tr>\n    <tr>\n      <th>5 colors</th>\n      <td>0.002596</td>\n      <td>0.000838</td>\n      <td>0.000033</td>\n      <td>0.000012</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#92c95bcf .cell execution_count=17}\n``` {.python .cell-code}\nprint(\"PSNR for Different Cases\")\nmse_dataframe = pd.DataFrame(pses,index = ['1 color','2 colors','5 colors'],columns = rs)\nmse_dataframe\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPSNR for Different Cases\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=17}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>5</th>\n      <th>10</th>\n      <th>25</th>\n      <th>50</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1 color</th>\n      <td>98.645203</td>\n      <td>102.114319</td>\n      <td>99.283699</td>\n      <td>96.905663</td>\n    </tr>\n    <tr>\n      <th>2 colors</th>\n      <td>65.851326</td>\n      <td>77.211937</td>\n      <td>90.339439</td>\n      <td>93.919067</td>\n    </tr>\n    <tr>\n      <th>5 colors</th>\n      <td>50.803440</td>\n      <td>60.592869</td>\n      <td>88.600624</td>\n      <td>97.540192</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n",
    "supporting": [
      "ImageReconstruction_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}
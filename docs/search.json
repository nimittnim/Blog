[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Junior Undergraduate, IIT Gandhinagar Computer Science and Engineering"
  },
  {
    "objectID": "posts/SVD/SVD.html",
    "href": "posts/SVD/SVD.html",
    "title": "Singular Value Decomposition (SVD)",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\nfrom matplotlib.image import imread\nplt.rcParams['figure.figsize']=[16,8]\n\nA=imread(\"Datasets/puppy.jpg\")\nX=np.mean(A,-1)\nimg = plt.imshow(X)\nplt.set_cmap('gray')\nplt.show()\n\n\n\n\n\n\n\n\n\nU,S,VT=np.linalg.svd(X,full_matrices=False)\nnp.shape(VT)\nS=np.diag(S)\nfor r in [5,20,100,340]:\n    Xapprox=U[:,:r] @ S[:r,:r] @ VT[:r,:]\n    print(np.shape(Xapprox))\n    plt.imshow(Xapprox)\n    plt.show()\n\n(340, 510)\n\n\n\n\n\n\n\n\n\n(340, 510)\n\n\n\n\n\n\n\n\n\n(340, 510)\n\n\n\n\n\n\n\n\n\n(340, 510)\n\n\n\n\n\n\n\n\n\n\nprint(np.shape(X))\nprint(np.shape(U),np.shape(S),np.shape(VT))\nprint(np.shape(U[:,:r]),np.shape(S[:r,:r]),np.shape(VT[:r,:]))\n\n(340, 510)\n(340, 340) (340, 340) (340, 510)\n(340, 340) (340, 340) (340, 510)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Hugging Face\n\n\n\n\n\n\nMachine Learning\n\n\n\nModel deployment using Hugging Face\n\n\n\n\n\nMar 15, 2024\n\n\nNimitt\n\n\n\n\n\n\n\n\n\n\n\n\nWord Embeddings\n\n\n\n\n\n\nMachine Learning\n\n\n\nOverview of WordEmbeddings Algorithm\n\n\n\n\n\nApr 15, 2023\n\n\nNimitt\n\n\n\n\n\n\n\n\n\n\n\n\nNoise Cancellation with FIR Filter\n\n\n\n\n\n\nSignal Processing\n\n\n\nUsing FIR filter to filter noise\n\n\n\n\n\nMar 15, 2023\n\n\nNimitt\n\n\n\n\n\n\n\n\n\n\n\n\nLinear Regression\n\n\n\n\n\n\nMachine Learning\n\n\n\nLinear Regression algorithm\n\n\n\n\n\nMar 15, 2023\n\n\nNimitt\n\n\n\n\n\n\n\n\n\n\n\n\nMatrix Factorization\n\n\n\n\n\n\nMachine Learning\n\n\n\nLower Represention of a matrix\n\n\n\n\n\nMar 15, 2023\n\n\nNimitt\n\n\n\n\n\n\n\n\n\n\n\n\nImage Reconstruction\n\n\n\n\n\n\nMachine Learning\n\n\n\nImage Reconstruction using Linear Regression\n\n\n\n\n\nMar 15, 2023\n\n\nNimitt\n\n\n\n\n\n\n\n\n\n\n\n\nPositional Encoding\n\n\n\n\n\n\nMachine Learning\n\n\n\nVector representaion of Text\n\n\n\n\n\nMar 15, 2023\n\n\nNimitt\n\n\n\n\n\n\n\n\n\n\n\n\nGradient Decent with Momentum\n\n\n\n\n\n\nMachine Learning\n\n\n\nAccelerating Gradient Decent using Momentum\n\n\n\n\n\nMar 15, 2023\n\n\nNimitt\n\n\n\n\n\n\n\n\n\n\n\n\nRegularized Linear Regression\n\n\n\n\n\n\nMachine Learning\n\n\n\nConstrained Optimisation\n\n\n\n\n\nMar 15, 2023\n\n\nNimitt\n\n\n\n\n\n\n\n\n\n\n\n\nRecurrent Neural Networks\n\n\n\n\n\n\nMachine Learning\n\n\n\nOverview of RNNs\n\n\n\n\n\nMar 15, 2023\n\n\nNimitt\n\n\n\n\n\n\n\n\n\n\n\n\nConvolutional Neural Network\n\n\n\n\n\n\nMachine Learning\n\n\n\nConvolutional Neural Network\n\n\n\n\n\nMar 15, 2023\n\n\nNimitt\n\n\n\n\n\n\n\n\n\n\n\n\nTransformer\n\n\n\n\n\n\nMachine Learning\n\n\n\nImplementation of a Transformer model\n\n\n\n\n\nMar 15, 2023\n\n\nNimitt\n\n\n\n\n\n\n\n\n\n\n\n\nseq2seq\n\n\n\n\n\n\nMachine Learning\n\n\n\nImplementation of a simple seq2seq model\n\n\n\n\n\nMar 15, 2023\n\n\nNimitt\n\n\n\n\n\n\n\n\n\n\n\n\nImproving Linear Regression using Random Fourier Feautures\n\n\n\n\n\n\nMachine Learning\n\n\n\nConvolutional Neural Network\n\n\n\n\n\nMar 15, 2023\n\n\nNimitt\n\n\n\n\n\n\n\n\n\n\n\n\nGradient Decent\n\n\n\n\n\n\nMachine Learning\n\n\n\nOptimisation using Gradient Decent\n\n\n\n\n\nMar 15, 2023\n\n\nNimitt\n\n\n\n\n\n\n\n\n\n\n\n\nLagranges Multiplier\n\n\n\n\n\n\nMachine Learning\n\n\n\nConstrained Optimisation\n\n\n\n\n\nMar 15, 2023\n\n\nNimitt\n\n\n\n\n\n\n\n\n\n\n\n\nBasis Functions\n\n\n\n\n\n\nMachine Learning\n\n\n\nBasis Functions for Linear Regression\n\n\n\n\n\nMar 12, 2023\n\n\nNimitt\n\n\n\n\n\n\n\n\n\n\n\n\nSingular Value Decomposition (SVD)\n\n\n\n\n\n\nMaths for ML\n\n\n\nExploring Singular Value Decomposition\n\n\n\n\n\nMar 7, 2023\n\n\nNimitt\n\n\n\n\n\n\n\n\n\n\n\n\nFeed Forward Neural Network\n\n\n\n\n\n\nMachine Learning\n\n\n\nImplementation of Feed Forward Neural Network from scratch\n\n\n\n\n\nFeb 15, 2023\n\n\nNimitt\n\n\n\n\n\n\n\n\n\n\n\n\nEnsemble Learning\n\n\n\n\n\n\nMachine Learning\n\n\n\nParadigm of Ensemble Learning\n\n\n\n\n\nFeb 10, 2023\n\n\nNimitt\n\n\n\n\n\n\n\n\n\n\n\n\nMid Point Method\n\n\n\n\n\n\nNumerical Methods\n\n\n\nImplementation of Mid Point Method\n\n\n\n\n\nJan 15, 2023\n\n\nNimitt\n\n\n\n\n\n\n\n\n\n\n\n\nHeun Method\n\n\n\n\n\n\nNumerical Methods\n\n\n\nImplementation of Heun Method\n\n\n\n\n\nJan 15, 2023\n\n\nNimitt\n\n\n\n\n\n\n\n\n\n\n\n\nGauss Elimination\n\n\n\n\n\n\nNumerical Methods\n\n\n\nImplementation of Gauss Elimination\n\n\n\n\n\nJan 15, 2023\n\n\nNimitt\n\n\n\n\n\n\n\n\n\n\n\n\nLagrange Interpolation\n\n\n\n\n\n\nNumerical Methods\n\n\n\nImplementation of Lagrange Interpolation\n\n\n\n\n\nJan 15, 2023\n\n\nNimitt\n\n\n\n\n\n\n\n\n\n\n\n\nTDMA\n\n\n\n\n\n\nNumerical Methods\n\n\n\nImplementation of TDMA\n\n\n\n\n\nJan 15, 2023\n\n\nNimitt\n\n\n\n\n\n\n\n\n\n\n\n\nEulers Method\n\n\n\n\n\n\nNumerical Methods\n\n\n\nImplementation of Eulers Method\n\n\n\n\n\nJan 15, 2023\n\n\nNimitt\n\n\n\n\n\n\n\n\n\n\n\n\nDecision Tree\n\n\n\n\n\n\nMachine Learning\n\n\n\nDecision Tree Algorithm\n\n\n\n\n\nJan 15, 2023\n\n\nNimitt\n\n\n\n\n\n\n\n\n\n\n\n\nGauss Siedel\n\n\n\n\n\n\nNumerical Methods\n\n\n\nImplementation of Gauss Sielel\n\n\n\n\n\nJan 15, 2023\n\n\nNimitt\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/FeedForwardNeuralNetworks/FeedForwardNeuralNetworks.html",
    "href": "posts/FeedForwardNeuralNetworks/FeedForwardNeuralNetworks.html",
    "title": "Feed Forward Neural Network",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nActivation Functions\n\n# Heaviside step function \n\ndef heavyside(x):\n    if (x &gt;= 0):\n        return 1\n    else :\n        return 0\n        \n# Rectified error Linear Unit function\n\ndef relu(x):\n    return max(0,x)\n\n# Sigmoid function\n\ndef activation_sigmoid(t):\n        return 1/(1+np.exp(-t))\n\ndef sigmoid(x):\n    return 1/(1+np.exp(-x))\n\ndef softmax(t):\n    return np.exp(t)/np.sum(np.exp(t))\n\n# Gassian error Linear Unit GeLU function\n\nNeural Network\n\n# Dataset\ndata = load_diabetes()\nX = data['data']\ny = data['target']\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)\n\n\nclass NeuralNetwork :\n\n    def __init__(self):\n        self.layers_wieghts = []\n        self.layers_bias = []\n        self.layers_activations = []  \n\n    def activation_sigmoid(self, t):\n        return 1/(1+np.exp(-t))\n\n    def activation_sigmoid_derivative(self, t):\n        return np.exp(-t)/(1+np.exp(-t))**2\n    \n    def activation_relu(self,t):\n        return np.maximum(0,t)\n    \n    def activation_relu_derivative(self,t):\n        return np.where(t &lt;= 0, 0, 1)\n\n    def activation_identity(self,t):\n        return t\n    \n    def activation_identity_derivative(self,t):\n        return 1\n\n    def loss(self, X_train, y_train):\n        y_hat = self.out(X_train)\n        return mean_squared_error(y_hat, y_train)\n\n    def loss_for_a_sample(self,x,y):\n        y_hat = self.out(x)\n        return np.mean((y_hat-y)**2)\n                                  \n    def integrate_layer(self,layer_index, x):\n        out = self.layers_wieghts[layer_index]@x + self.layers_bias[layer_index]\n        if (self.layers_activations[layer_index] == 's'):\n            return  self.activation_sigmoid(out)\n        elif (self.layers_activations[layer_index] == 'r'):\n            return  self.activation_relu(out)\n        elif (self.layers_activations[layer_index] == 'i'):\n            return  self.activation_identity(out)\n    \n    def add_layer(self, layer_size, activation):\n\n        if len(self.layers_activations) == 0:\n            self.layers_wieghts.append(np.random.rand(layer_size, layer_size))\n            self.layers_bias.append(np.random.rand(layer_size))\n            self.layers_activations.append(activation)\n        else :\n            self.layers_wieghts.append(np.random.rand(layer_size, len(self.layers_bias[-1])))\n            self.layers_bias.append(np.random.rand(layer_size))\n            self.layers_activations.append(activation)\n        \n    def update_weights(self, layer_index, delta_w):\n        self.layers_wieghts[layer_index]  = self.layers_wieghts[layer_index]  + delta_w\n\n    def update_bias(self, layer_index, delta_b):\n        self.layers_bias[layer_index]  = self.layers_bias[layer_index] + delta_b \n\n    def delete_layer(self):\n        if len(self.layers) &gt; 1:\n            self.layers.pop()\n        else :\n            print(\"No Hidden Layers to delete\")\n\n    def derivative_of_activation_step(self,E, layer_index):\n        if self.layers_activations[layer_index] == 's':\n            return self.activation_sigmoid_derivative(E)\n        elif self.layers_activations[layer_index] == 'r':\n            return self.activation_relu_derivative(E)\n        elif self.layers_activations[layer_index] == 'i':\n            return self.activation_identity_derivative(E)\n\n    def backward(self, X_, y_, alpha):\n        activations = self.get_activations(X_)\n        l = len(self.layers_bias)\n\n        # Initiating with Output Layer\n        E_d = activations[l-1] - y_\n        E_d = E_d*self.derivative_of_activation_step(E_d, l-1)\n        delta_w = -(alpha/len(E_d))*2* E_d @ activations[l-2].T.reshape(-1, len(activations[l-2]))\n        delta_b = -(alpha/len(E_d))*2* E_d\n        delta_a_l = -alpha*2*E_d @ self.layers_wieghts[l-1]\n        delta_a_l = delta_a_l*self.derivative_of_activation_step(delta_a_l,l-1)\n        self.update_weights(l-1, delta_w)\n        self.update_bias(l-1, delta_b)\n        # print(activations)\n        # print(\"\\n\")\n        \n        # Hidden Layers\n        for i in range(l-2,0,-1):\n            delta_w = -((alpha/len(delta_a_l))*2* delta_a_l).reshape(len(delta_a_l),1) @ activations[i-1].T.reshape(-1, len(activations[i-1]))\n            delta_b = -(alpha/len(delta_a_l))*2* delta_a_l\n            self.update_weights(i, delta_w)\n            self.update_bias(i, delta_b)\n            delta_a_l = -(alpha/len(delta_a_l))*2*delta_a_l @ self.layers_wieghts[i]\n            delta_a_l = delta_a_l*self.derivative_of_activation_step(delta_a_l,i)\n        \n        # Input Layer\n        delta_w = -(alpha/len(delta_a_l))*2* delta_a_l.reshape(len(delta_a_l),1) @ X_.T.reshape(-1,len(X_))\n        delta_b = -(alpha/len(delta_a_l))*2* delta_a_l\n        self.update_weights(0, delta_w)\n        self.update_bias(0, delta_b)\n    \n    def get_activations(self, X_):\n        l = len(self.layers_bias)\n        activations = []\n        previous_out = self.integrate_layer(0, X_).copy()\n        activations.append(previous_out)\n        for i in range(1,l):\n            current_out = self.integrate_layer(i,previous_out).copy()\n            previous_out = current_out.copy()\n            activations.append(previous_out)\n        return activations\n\n    def forward(self, X_):\n        l = len(self.layers_bias)\n        previous_out = self.integrate_layer(0, X_).copy()\n        for i in range(1,l):\n            current_out = self.integrate_layer(i,previous_out).copy()\n            previous_out = current_out.copy()\n        return previous_out\n    \n    def train(self,alpha, X_train, y_train):\n        max_epochs = 1000\n        for e in range(max_epochs):\n            # out = self.predict(X_train)\n            # print(mean_squared_error(out,y_train),out[0])\n            for i in range(len(X_train)):\n                self.backward(X_train[i],y_train[i],alpha)\n            \n    def predict(self, X_test):\n        n = len(X_test)\n        m = len(self.layers_bias[-1])\n        prediction = np.empty((n,m))\n        for i in range(n):\n            out = self.forward(X_test[i])\n            for j in range(m):\n                prediction[i][j] = out[j]\n        return prediction\n\n\nnn = NeuralNetwork()\nnn.add_layer(10,'r')\nnn.add_layer(1,'i')\n\n\nnn.forward(X_train[4])\n\n\nnn.train(0.0001, X_train, y_train) \n\n\ny_hat = nn.predict(X_test)\n\n\n# Plotting\nsr = np.arange(len(y_test))\nplt.plot(sr,y_test)\nplt.plot(sr,y_hat)\nplt.show()\n\n\nmean_squared_error(y_hat, y_test)\n\nPyTorch Implementation\n\n# Dataset\ndata = load_diabetes()\nX = data['data']\ny = data['target']\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)\n\nX_train = torch.tensor(X_train, dtype=torch.float32)\ny_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\nX_test= torch.tensor(X_test, dtype=torch.float32)\ny_test = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)\n\n# Model\nmodel = nn.Sequential(\n    nn.Linear(10, 12),\n    nn.ReLU(),\n    nn.Linear(12, 8),\n    nn.ReLU(),\n    nn.Linear(8, 1),\n)\nprint(model)\n \n# train the model\nloss_fn   = nn.MSELoss()  \noptimizer = optim.Adam(model.parameters(), lr=0.001)\n \nn_epochs = 100\nbatch_size = 10\n \nfor epoch in range(n_epochs):\n    for i in range(0, len(X_train), batch_size):\n        Xbatch = X_train[i:i+batch_size]\n        y_pred = model(Xbatch)\n        ybatch = y_train[i:i+batch_size]\n        loss = loss_fn(y_pred, ybatch)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    print(f'Finished epoch {epoch}, current loss {loss}')\n\n\ny_hat = model(X_test)\n\n\n# Plotting\nsr = np.arange(len(y_test))\nplt.plot(sr, np.array(y_test))\nplt.plot(sr,y_hat.detach().numpy())\nplt.show()"
  },
  {
    "objectID": "posts/MatrixFactorization/MatrixFactorization.html",
    "href": "posts/MatrixFactorization/MatrixFactorization.html",
    "title": "Matrix Factorization",
    "section": "",
    "text": "Matrix Completion\nWe intend to complete the Matrix with unkown entries using Matrix decomposotion where we learn decomposed matrices using Gradient Decent and Nested Linear Regression\nMatrix Factorization\nFor a matrix \\(A_{n,m}\\) learn \\(W_{n,r}\\) and \\(H_{r,m}\\) such that,\n$ A = WH$\nMethod 1 : Gradient Decent\n\nimport numpy as  np\nimport matplotlib.pyplot as plt\n\n\n# Creating A\nfrom matplotlib.image import imread\n\nX = imread(\"Datasets/puppy.jpg\")\nA = np.mean(X,-1)\nimg = plt.imshow(A)\nplt.set_cmap('gray')\nplt.show()\n\n\n# Normalizing the data\n\nA_transformed = A/255\n\n\nimg = plt.imshow(A_transformed)\nplt.set_cmap('gray')\nplt.show()\n\n\n# Training\n\n# Gradients\n\ndef loss_gradient_wrt_W(W,H,A):\n    lg_matrix = np.empty(W.shape)\n    N  = (A.shape[0]*A.shape[1])\n    for j1 in range(W.shape[0]):\n        M = A[j1] - W[j1]@H\n        for j2 in range(W.shape[1]): \n            lg_matrix[j1][j2] = -2*( M @ H[j2])/ N\n    return lg_matrix\n\ndef loss_gradient_wrt_H(W,H,A):\n    lg_matrix = np.empty(H.shape)\n    N  = (A.shape[0]*A.shape[1])\n    for j2 in range(H.shape[0]):\n        M = A[:,j2] - W@H[:,j2]\n        for j1 in range(H.shape[1]):\n            lg_matrix[j1][j2] = -(2* M @ W[:,j1])/ N\n    return lg_matrix\n\n\ndef loss (A, W, H):\n    return np.sum((A - W@H)**2)/(A.shape[0]*A.shape[1])\n\n# intializing W and H\nr = 100\nn = A.shape[0]\nm = A.shape[1]\nW = 0.01*np.random.rand(n,r)\nH = 0.01*np.random.rand(r,m)\n\nmax_iterations = 200\n\nalpha = 150\n\nfor i in range(max_iterations):\n    W = W - alpha*loss_gradient_wrt_W(W,H,A_transformed)\n    H = H - alpha*loss_gradient_wrt_H(W,H,A_transformed)\n    img = plt.imshow(W@H)\n    if(i%5 == 0):\n        print(loss(A_transformed, W,H))\n        print(max((W@H)[0]))\n        plt.set_cmap('gray')\n        plt.show()\n\n\nA_constructed = W@H\n\n\nimg = plt.imshow(A_constructed)\nplt.set_cmap('gray')\nplt.show()\n\nIf the given matrix A is incomplete, we simply don’t take into account those elements in thee loss function for learning W and H. After learning W and H, the completed version of A is W @ H.\nMethod 2 : Alternating Least Squares\nLet’s first use the A matrix as it is :\n\n# intializing W and H\nr = 100\nn = A.shape[0]\nm = A.shape[1]\nW = np.random.rand(n,r)\nH = np.random.rand(r,m)\n\n# training\n\nmax_iterations = 100\n\nfor _ in range(max_iterations):\n    # learning H\n    for i in range(H.shape[1]):\n        H[:,i] = np.linalg.inv((W.T@W))@W.T@A[:,i]\n    # learning W\n    for j in range(W.shape[0]):\n        W[j] = np.linalg.inv(H@H.T)@H@A[j]\nconstructed_A = W@H\n\n\nimg = plt.imshow(constructed_A)\nplt.set_cmap('gray')\nplt.show()\n\nDropping some values in A :\n\nmask = np.random.rand(A.shape[0], A.shape[1])\nfor i in range(A.shape[0]):\n    for j in range(A.shape[1]):\n        if mask[i][j] &lt; 0.5:\n            mask[i][j] = 1\n        else :\n            mask[i][j] = 0\nM = np.ma.masked_array(A,mask)\n\n\nA_incomplete = M.filled(np.nan)\n\n\nimg = plt.imshow(A_incomplete)\nplt.set_cmap('gray')\nplt.show()\n\n\ndef plot_image(A):\n    plt.imshow(A)\n    plt.set_cmap('gray')\n    plt.show()\n\n\n# Reconstructing\n\n# intializing W and H\nr = 100\nn = A.shape[0]\nm = A.shape[1]\nW = np.random.rand(n,r)\nH = np.random.rand(r,m)\n\n# training\n\nmax_iterations = 3\n\nfor _ in range(max_iterations):\n    # learning H\n    for i in range(H.shape[1]):\n        keep = ~np.isnan(A_incomplete[:,i])\n        M = np.linalg.inv((W.T@W))@W.T\n        for i1 in range(H.shape[0]):\n            H[:,i][i1] = M[i1][keep]@A_incomplete[:,i][keep]\n    # learning W\n    for j in range(W.shape[0]):\n        keep = ~np.isnan(A_incomplete[j])\n        M = (np.linalg.inv(H@H.T)@H)\n        for j1 in range(W.shape[1]):\n            W[j][j1] = M[j1][keep]@A_incomplete[j][keep]\n    plot_image(W@H)\nconstructed_A = W@H\n\n\nplot_image(constructed_A)"
  },
  {
    "objectID": "posts/HuggingFace/HuggingFace.html",
    "href": "posts/HuggingFace/HuggingFace.html",
    "title": "Hugging Face",
    "section": "",
    "text": "%pip install -qU \"transformers==4.38.0\" --upgrade\n%pip install -qU \"huggingface_hub\"\n\n\n# from google.colab import userdata\n# from huggingface_hub import login\n# login(userdata.get('HF_TOKEN'))\n\nText Generation using Gemma 2B\n\nimport torch\n\n\nfrom transformers import pipeline\ngenerate = pipeline(task = \"text-generation\", model = \"google/gemma-2b-it\", model_kwargs = {\"torch_dtype\": torch.bfloat16},device = \"cuda\")\n\n\ninst = [\n    {\n        \"role\" : \"user\",\n        \"content\" : \"Define an algorithm\"\n    }\n]\nprompt = generate.tokenizer.apply_chat_template(inst,tokenize = False,add_generation_prompt = True)\n\n\nprint(prompt)\n\n\nout = generate(prompt, max_new_tokens = 200)\nprint(out)\n\n\nprint(out[0][\"generated_text\"][len(prompt):])\n\nText Classification\n\nclassifier = pipeline(task = \"sentiment-analysis\",model = \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\")\n\n\nclassifier(\"You are so kind\")\n\n\nclassifier(\"You should not behave rudely\")\n\n\nclassifier(\"Everything is happening worst in my life\")"
  },
  {
    "objectID": "posts/eulersMethod/eulersMethod.html",
    "href": "posts/eulersMethod/eulersMethod.html",
    "title": "Eulers Method",
    "section": "",
    "text": "x = []\ny = []\nt = []\nstep = 0.1\nt_end = 20\nfor k in range(int(t_end/step)):\n    t.append(k*step)\n\nx.append(2)\ny.append(1)\nx_previous  = 2\ny_previous = 1\ndef x_deri(x,y):\n    a = 1.2 \n    b = 0.6\n    x_deri = a*x - b*x*y\n    return x_deri\n\ndef y_deri(x,y):\n    c = 0.8\n    d = 0.3\n    y_deri =  -c*y + d*x*y\n    return y_deri\n\nfor i in range(len(t)):\n    y_current = y_previous + y_deri(x_previous,y_previous)*step\n    x_current = x_previous + x_deri(x_previous,y_previous)*step\n    y.append(y_current)\n    x.append(x_current)\n    y_previous = y_current\n    x_previous = x_current\n\nprint(\"y(20) : \",x[-1])\nprint(\"y(20)\",y[-1])"
  },
  {
    "objectID": "posts/LagrangeInterpolation/LagrangeInterpolation.html",
    "href": "posts/LagrangeInterpolation/LagrangeInterpolation.html",
    "title": "Lagrange Interpolation",
    "section": "",
    "text": "import matplotlib.pyplot as plt\n\n\ndef lagrangeInterpolation(x_array,y_array,x,n):\n    y = 0\n    for i in range(n):\n        L=1\n        for j in range(n):\n            if i!=j:\n                L *= (x - x_array[j])/(x_array[i]-x_array[j])\n        y+=(L*y_array[i])\n    return y\n\nx_array = [0,10,20,30]\ny_array = [10,35,55,52]\nx = 30\ny = lagrangeInterpolation(x_array,y_array,x,len(x_array))\nprint(y)\n\n\n# 3/8 Simpson's rule for first four intervals\nd = 0.001\ndt = d\nt = 0\nI1 = 0\nT=[]\nc=[]\nx_array = [0,10,20,30]\ny_array = [10,35,55,52]\ny_0 = lagrangeInterpolation(x_array,y_array,t,len(x_array))\nwhile t &lt; 30+d:\n    c_ = lagrangeInterpolation(x_array,y_array,t,len(x_array))\n    I1 += c_*dt\n    T.append(t)\n    c.append(c_)\n    t+=dt\nprint(I1)\n\n\n# 1/3 Simpson's rule for second two intervals\ndt = d\nt = 30\nI2 = 0\nx_array = [30,35,40]\ny_array = [52,40,37]\ny_30 = lagrangeInterpolation(x_array,y_array,t,len(x_array))\nwhile t &lt; 40+d :\n    c_ = lagrangeInterpolation(x_array,y_array,t,len(x_array))\n    I2 += c_*dt\n    T.append(t)\n    c.append(c_)\n    t+=dt\nprint(I2)\n\n\n# 1/3 Simpson's rule for third two intervals\ndt = d\nt = 40\nI3 = 0\nx_array = [40,45,50]\ny_array = [37,32,34]\ny_40 = lagrangeInterpolation(x_array,y_array,t,len(x_array))\nwhile t &lt; 50+d:\n    c_ = lagrangeInterpolation(x_array,y_array,t,len(x_array))\n    I3 += c_*dt\n    T.append(t)\n    c.append(c_)\n    t+=dt\ny_50 = lagrangeInterpolation(x_array,y_array,t,len(x_array))\nprint(I3)\n\n\nx_p = [0,30,40,50]\ny_p = [y_0,y_30,y_40,y_50]\nI_arr = [I1,I2,I3]\nfor i in range(len(x_p)-1):\n    plt.text((x_p[i] + x_p[i+1]) / 2.2, 20,f\"{I_arr[i]:.2f}\",fontsize=11)\nplt.plot(T,c)\nplt.xlabel(\"t\")\nplt.ylabel(\"c\")\nplt.title(\"c vs t\")\nfor j in range(len(x_p)):\n    plt.plot((x_p[j],x_p[j]),(0,y_p[j]),color = 'orange')\n\n\nprint(\"Final Answer is : \",(I1+I2+I3)*4)"
  },
  {
    "objectID": "posts/HeunMethod/HeunMethod.html",
    "href": "posts/HeunMethod/HeunMethod.html",
    "title": "Heun Method",
    "section": "",
    "text": "x = []\ny = []\nt = []\nstep = 0.1\nt_end = 20\nfor k in range(int(t_end/step)):\n    t.append(k*step)\na = 1.2 \nb = 0.6\nc = 0.8\nd = 0.3\nx.append(2)\ny.append(1)\nx_previous  = 2\ny_previous = 1\n\ndef x_deri(x,y):\n    a = 1.2 \n    b = 0.6\n    x_deri = a*x - b*x*y\n    return x_deri\n\ndef y_deri(x,y):\n    c = 0.8\n    d = 0.3\n    y_deri =  -c*y + d*x*y\n    return y_deri\n\nfor i in range(len(t)):\n    y_current = y_previous + y_deri(x_previous,y_previous)*step\n    y_corrected_deri = (y_deri(x_previous,y_previous)+y_deri(x_previous,y_current))/2\n    y_current = y_previous + y_corrected_deri*step\n\n    x_current = x_previous + x_deri(x_previous,y_previous)*step\n    x_corrected_deri = (x_deri(x_previous,y_previous)+x_deri(x_current,y_previous))/2\n    x_current = x_previous + x_corrected_deri*step\n\n    y.append(y_current)\n    x.append(x_current)\n    y_previous = y_current\n    x_previous = x_current\n\nprint(x[-1])\nprint(y[-1])\nprint(len(x))"
  },
  {
    "objectID": "posts/MidPointMethod/MidPointMethod.html",
    "href": "posts/MidPointMethod/MidPointMethod.html",
    "title": "Mid Point Method",
    "section": "",
    "text": "x = []\ny = []\nt = []\nstep = 0.1\nt_end = 20\nfor k in range(int(t_end/step)):\n    t.append(k*step)\na = 1.2 \nb = 0.6\nc = 0.8\nd = 0.3\nx.append(2)\ny.append(1)\nx_previous  = 2\ny_previous = 1\n\ndef x_deri(x,y):\n    a = 1.2 \n    b = 0.6\n    x_deri = a*x - b*x*y\n    return x_deri\n\ndef y_deri(x,y):\n    c = 0.8\n    d = 0.3\n    y_deri =  -c*y + d*x*y\n    return y_deri\n\nfor i in range(len(t)):\n    y_half = y_previous + y_deri(x_previous,y_previous)*(step/2)\n    x_half = x_previous + x_deri(x_previous,y_previous)*(step/2)\n\n    y_current = y_half + y_deri(x_half,y_half)*step/2\n    x_current = x_half + x_deri(x_half,y_half)*step/2\n\n    y.append(y_current)\n    x.append(x_current)\n    y_previous = y_current\n    x_previous = x_current\n\nprint(x[-1])\nprint(y[-1])\nprint(len(x))"
  },
  {
    "objectID": "posts/GaussElimination/GaussElimination.html",
    "href": "posts/GaussElimination/GaussElimination.html",
    "title": "Gauss Elimination",
    "section": "",
    "text": "import numpy as np\n\nn=int(input(\"Enter n\"))\nmat=np.zeros((n,n+1))\nroots=np.zeros(n+1)\n\ndef forwardElimination(mat):\n    for j in range(n):\n        row = input(\"Enter the first row of coefficients\")\n        row_arr=row.split(\" \")\n        for l in range(n):\n            mat[j,l]=row_arr[l]\n        mat[j,n]=row_arr[n]\n\n    j=0\n    while j&lt;n :\n        for k in range(j+1,n) :\n            mat[k]=mat[k] - (mat[k,j]/mat[j,j])*mat[j]\n        j+=1\nforwardElimination(mat)\n\ndef backSubstitution(mat) : \n    global roots\n    roots[n-1]=mat[n-1,n]/mat[n-1][n-1]\n    p=n-2\n    while p&gt;-1:\n        roots[p] = (mat[p,n] - sum((np.multiply(mat[p],roots))))/(mat[p,p])\n        p-=1\n\nbackSubstitution(mat)\nprint(\"RRF :\",mat)\nprint(\"Zeros are :\",roots[0:n])\n\n\nimport numpy as np\n\nn=int(input(\"Enter n\"))\nmat=np.zeros((n,n+1))\nroots=np.zeros(n+1)\n\ndef forwardElimination(mat):\n    for j in range(n):\n        row = input(\"Enter the first row of coefficients\")\n        row_arr=row.split(\" \")\n        for l in range(n):\n            mat[j,l]=row_arr[l]\n        mat[j,n]=row_arr[n]\n\n    j=0\n    while j&lt;n :\n        for k in range(j+1,n) :\n            mat[k]=mat[k] - (mat[k,j]/mat[j,j])*mat[j]\n        j+=1\nforwardElimination(mat)\n\ndef backSubstitution(mat) : \n    global roots\n    roots[n-1]=mat[n-1,n]/mat[n-1][n-1]\n    p=n-2\n    while p&gt;-1:\n        roots[p] = (mat[p,n] - sum((np.multiply(mat[p],roots))))/(mat[p,p])\n        p-=1\n\nbackSubstitution(mat)\nprint(\"RRF :\",mat)\nprint(\"Zeros are :\",roots[0:n])\n\n\nimport numpy as np\n\nn=int(input(\"Enter n\"))\nmat=np.zeros((n,n+1))\nroots=np.zeros(n+1)\n\ndef forwardElimination(mat):\n    for j in range(n):\n        row = input(\"Enter the first row of coefficients\")\n        row_arr=row.split(\" \")\n        for l in range(n):\n            mat[j,l]=row_arr[l]\n        mat[j,n]=row_arr[n]\n\n    j=0\n    while j&lt;n :\n        for k in range(j+1,n) :\n            mat[k]=mat[k] - (mat[k,j]/mat[j,j])*mat[j]\n        j+=1\nforwardElimination(mat)\n\ndef backSubstitution(mat) : \n    global roots\n    roots[n-1]=mat[n-1,n]/mat[n-1][n-1]\n    p=n-2\n    while p&gt;-1:\n        roots[p] = (mat[p,n] - sum((np.multiply(mat[p],roots))))/(mat[p,p])\n        p-=1\n\nbackSubstitution(mat)\nprint(\"RRF :\",mat)\nprint(\"Zeros are :\",roots[0:n])"
  },
  {
    "objectID": "posts/TDMA/tdma.html",
    "href": "posts/TDMA/tdma.html",
    "title": "TDMA",
    "section": "",
    "text": "#This code genrates L and U and then subctitutes to get roots\nn = int(input(\"Enter n\"))\n\nmat=[]\nfor i in range(n):\n    mat.append([0.0] * (n))\n\n\n\nfor j in range(n):\n    row = input(f\"Enter the {j + 1} th row of coefficient Matrix\")\n    row_arr = row.split(\" \")\n    for l in range(n):\n        mat[j][l] = float(row_arr[l])\nprint(\"Entered Matrix : \\n\")\nfor m in range(n):\n    print(mat[m])\nL = []\nfor i in range(n):\n    L.append([0.0] * (n))\nfor i in range(n):\n    L[i][i] = 1\n\nU = []\nfor i in range(n):\n    U.append([0.0] * (n))\n\ndef forwardElimination(mat,L,U):\n    for j in range(n):\n        for k in range(j+1,n) :\n            f_kj = mat[k][j]/mat[j][j]\n            L[k][j] = - f_kj\n            for l in range(n):\n                mat[k][l]=mat[k][l] - (f_kj)*mat[j][l]\n    for q in range(n):\n        for p in range(n):\n            U[q][p] = mat[q][p]\n\nforwardElimination(mat,L,U)\nprint('\\n')\nprint(\"L is : \")\nfor m in range(n):\n    print(L[m])\n\nprint('\\n')\nprint(\"U is : \")\nfor m in range(n):\n    print(U[m])"
  },
  {
    "objectID": "posts/GaussSiedel/GaussSiedel.html",
    "href": "posts/GaussSiedel/GaussSiedel.html",
    "title": "Gauss Siedel",
    "section": "",
    "text": "n = int(input(\"Enter n\"))\n\nmat = [[0.0] * (n + 1) for _ in range(n)]\n\nfor j in range(n):\n    row = input(f\"Enter the {j + 1} th row of coefficients\")\n    row_arr = row.split(\" \")\n    for l in range(n + 1):\n        mat[j][l] = float(row_arr[l])\n\nprint(\"Enter your guessed zeros:\\n\")\nroots = [0.0] * n\nroots_array = list(map(float, input(\"Enter the roots : \").split()))\n\n\nfor k in range(1, n):\n    roots[k] = roots_array[k - 1]\nprint(roots)\n\ndef GaussSiedel(mat, roots, eps=0.001, max_iterations=100):\n    n = len(mat)\n    eps = 0.0000001\n    \n    for iter in range(max_iterations):\n        flag = False\n        for i in range(n):\n            x = 0\n            for j in range(n):\n                if j != i:\n                    x += mat[i][j] * roots[j]\n            x = (mat[i][n] - x) / mat[i][i]\n            e = abs(roots[i] - x)\n            if e &gt; eps:\n                flag = True\n            roots[i] = x\n        if not flag:\n            break\n        print(roots)\n\n\nGaussSiedel(mat, roots)\nprint(\"Roots:\", roots)"
  }
]